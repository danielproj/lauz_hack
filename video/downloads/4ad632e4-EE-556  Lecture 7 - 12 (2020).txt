~EE-556 / Lecture 7 - 1/2 (2020)
~2020-10-20T11:13:24.632+02:00
~https://tube.switch.ch/videos/4ad632e4
~EE-556 Mathematics of data: from theory to computation
[0.0:8.96] All right, let's speak in.
[8.96:17.52] So what I'll do today is give your brief introduction to deep learning.
[17.52:26.76] For this purpose, we have a mixture of slides that give you some high level messages about
[26.76:28.72] deep learning.
[28.72:36.48] The hype, what you can do with it, how it fits into the statistical learning framework
[36.48:40.96] that we talked about so far in the lectures.
[40.96:46.4] And then I'll tell you a little bit about what the theory says about deep learning.
[46.4:58.44] And hopefully I will set up some mysteries for the next week's lecture.
[58.44:63.879999999999995] So here's the outline today.
[63.879999999999995:72.6] This Friday there will be a restitation, I believe, on some practical tricks of the
[72.6:77.0] trade for deep learning, which I think should be useful.
[77.0:78.0] Okay.
[78.0:91.16] Now, let me tell you by sharing something that has been troubling in the following
[91.16:92.16] sense.
[92.16:99.76] We've been very hard at work trying to set up a notation for this course.
[99.76:108.72] And for this, we used A as our data, B as our labels.
[108.72:113.92] We were using U for our biases.
[113.92:117.84] And our optimization variables go to this little X, right?
[117.84:121.2] So they will be the network parameters and so on and so forth.
[121.2:130.6] But in the deep learning literature, if you see this literature X is in fact the data,
[130.6:138.64000000000001] the sample, the labels are given typically by Y, the bias terms are B and the rates are
[138.64000000000001:139.64000000000001] doubly.
[139.64000000000001:146.48000000000002] So there is some sort of a mapping between two notations, which you have it here.
[146.48:152.28] So hopefully this will make it clear when you read the deep learning literature and look
[152.28:155.48] at our lectures and vice versa.
[155.48:157.92] All right.
[157.92:165.16] Now I'll begin by talking about a stylized example of classification.
[165.16:170.92] And I'll try to somehow pull the rug underneath.
[170.92:181.39999999999998] So we talked about logistic regression and here we are given some samples, AI, where
[181.39999999999998:187.07999999999998] AI is some feature vector and EIs are plus 1 minus 1 labels.
[187.07999999999998:197.51999999999998] And what we did was to model, this is a variable for the labels and use the logistic score function
[197.52:207.44] for linear score functions and logistic link function to the assigned to obtain probabilities.
[207.44:216.56] In this case, take a look at these two class examples in one, you know, these cross-versus
[216.56:226.8] dots where you see that by using a hyperplane, you can in fact perfectly separate these two
[226.8:227.8] classes.
[227.8:229.68] All right.
[229.68:235.8] If you recall what we did with the logistic regression in the end, we we have an estimate,
[235.8:236.8] right.
[236.8:245.12] And then we used the set estimate given a new sample, you would take an inner product and
[245.12:249.4] we would look at whether or not this inner product was greater than 0.
[249.4:250.4] All right.
[250.4:254.84] And in this particular case, if you think about it, suppose you learned this type of plane,
[254.84:260.84000000000003] you're going to need data points, you just need to look at whether or not this particular
[260.84000000000003:266.48] inner product is positive or negative to decide on whether the new data point is across
[266.48:268.0] or a certain.
[268.0:272.08] Unfortunately, no matter how you try here, right.
[272.08:278.96] This is not possible, this particular example because the data lives on this manifold, it
[278.96:284.32] looks like a donut from this particular angle.
[284.32:289.32] And this is not a linearly separable set.
[289.32:297.8] So this one is a linearly separable set, whereas these are not.
[297.8:298.8] All right.
[298.8:308.12] So ideally, you can already imagine another set in the team like a circle that can separate
[308.12:311.44] them, but that's not what we're doing with the logistic regression.
[311.44:316.88] At least the way these set them up.
[316.88:320.44] Okay.
[320.44:328.24] At this point, I mean, it may look like all hope was lost, but don't underestimate the
[328.24:329.92] power of linear classifiers.
[329.92:330.92] All right.
[330.92:338.15999999999997] So what I'll talk about now is a trick that comes to the rescue.
[338.16:344.20000000000005] This has been the trick for the long this time.
[344.20000000000005:351.08000000000004] The idea is that given the data, you'll try to come up with other features and lift the
[351.08000000000004:353.48] dimension of the problem.
[353.48:359.88] So here, what I will do is suppose we have, you know, A x by 1, A 2.
[359.88:364.28000000000003] Think about this as a third dimension.
[364.28:371.2] And create another feature that just loops at the, so imagine in this particular case,
[371.2:375.08] the origin is here.
[375.08:376.08] You know.
[376.08:383.08] So in this case, what we're using is the distance to the origin is yet another feature.
[383.08:400.08] If given this new, let's say augmented data, you can see that this particular donut
[400.08:403.96] figure becomes like an ice cream cone.
[403.96:409.52] And in fact, in the third dimension, it looks like it's linearly separable or just a simple
[409.52:411.0] hyperplane.
[411.0:415.2] As it's amenable to the tricks of the trade in logistic regression that we talked about
[415.2:416.72] so far.
[416.72:422.76] Moreover, we know that the logistic regression is a nice comics program.
[422.76:430.44] So by just adding another dimension, we made the problem a comics problem.
[430.44:436.64] But of course, and this in this particular case, the data is separable so everything
[436.64:440.56] is good and then the, but in general, this, this trick.
[440.56:445.08] It may also have some side effects.
[445.08:451.44] And the particular side effect is the course of dimensionality in the sense that given the
[451.44:457.08] original problem, you may have to lift the problems in even higher dimensions.
[457.08:468.6] So this nice comics optimization objective.
[468.6:476.92] He is a bit harder to solve than maybe applying gradient descent in the original ambient dimension.
[476.92:481.36] So this is what we call as the course of dimensionality.
[481.36:492.16] And the course of dimensionality phrase will reappear later on.
[492.16:502.08000000000004] They're based to avoid certain issues in the course of dimensionality, but typically,
[502.08000000000004:507.0] it's typically the number of data points becomes an issue.
[507.0:511.40000000000003] But even then, now there are methods to speed these things up and make the complexity
[511.40000000000003:513.4] a bit better.
[513.4:515.4] Okay.
[515.4:525.6] Now, what I will do is to give you a very important alternative for the particular case of
[525.6:527.88] classification problem.
[527.88:534.12] And this particular construction will be applicable to regression and also the estimation, as we
[534.12:536.4] will see in the upcoming elections.
[536.4:545.0799999999999] Now, for this, what we will do is introduce a single hidden layer neural network with
[545.08:549.44] M neurons.
[549.44:558.9200000000001] As you will see that some of the naming is from the literature.
[558.9200000000001:562.0400000000001] We want to understand what they imply.
[562.0400000000001:568.84] They have very precise meanings and not necessarily, I mean, there's some analogy with the
[568.84:572.12] brain, but these are very precise things.
[572.12:576.92] So, what we will do is we're going to use some weight vectors.
[576.92:578.92] Alright, so x1, x2.
[578.92:580.92] And then we will have some bias.
[580.92:583.92] And we will have something called an activation function.
[583.92:588.16] And I'll tell you a little bit about these in the next slide.
[588.16:597.28] And what you will do is if you remember, we want to form a parametric function to predict
[597.28:598.28] the labels.
[598.28:599.28] Alright.
[599.28:603.12] So, this is a and our parameters are x.
[603.12:607.48] And this is, again, where we deviate from the deep learning literature where the data
[607.48:615.04] input is typically called as x and the weights are typically double and the bias is rp.
[615.04:618.52] Just to be consistent with the lectures so far, we're using this notation.
[618.52:619.52] Alright.
[619.52:621.52] So, we have an input.
[621.52:625.68] So how do we find our prediction?
[625.68:632.12] We apply a weight matrix to the simple and then apply a bias.
[632.12:636.8399999999999] So, if you think about it, this is just an f i transformation and everything so far
[636.8399999999999:638.8399999999999] is linear.
[638.8399999999999:643.9599999999999] And then we apply something called an activation function.
[643.9599999999999:649.2399999999999] This activation function can be a general function, but typically it's a function that
[649.24:657.2] acts coordinate vise to the f i'm the transform input.
[657.2:658.2] Alright.
[658.2:665.16] So, if you think about it, you multiply weight with the input and then add bias.
[665.16:667.48] What we have here is a vector.
[667.48:668.48] Okay.
[668.48:671.6] So, just say b.
[671.6:675.72] And this activation function is a function of v.
[675.72:681.6] Generally, this can be any complicated function, but typically we use functions, activation
[681.6:685.44] functions that apply coordinate vise to the entries of v.
[685.44:686.44] Alright.
[686.44:690.64] And it gives you another vector.
[690.64:697.88] And then we have another weight that is applied after the activation.
[697.88:703.44] And I'll tell you the properties of activation is also important in the next slide because
[703.44:710.6400000000001] this particular construction is no longer linear due to the presence of the said activation
[710.6400000000001:711.6400000000001] function.
[711.6400000000001:712.6400000000001] Alright.
[712.6400000000001:716.8000000000001] So, activation function plays an important role in injecting non-linearity in the way
[716.8000000000001:721.2] we constructed this particular parametric function.
[721.2:723.2] Alright.
[723.2:725.6400000000001] And yeah, of course, we have another bias.
[725.6400000000001:726.6400000000001] Okay.
[726.6400000000001:733.4000000000001] Now, here, as far as our optimization problems will be concerned, we're going to have
[733.4:736.88] our parameters stacked up in this particular fashion.
[736.88:747.24] You can make this a vector of these parameters or matrix, whichever the purposes.
[747.24:760.1999999999999] So, our parameter so far is the weight vector inside the activation function, the weight
[760.2:767.2] matrix inside the activation function, the weight vector outside and the bias terms.
[767.2:774.32] So, this will be our one hidden layer neural network.
[774.32:783.88] If you want deeper layers, all you need to do is recursively apply this construction, meaning
[783.88:796.04] you apply the activation function to this, you multiply it in another weight matrix and
[796.04:800.96] then add a bias and so on and so forth.
[800.96:802.96] Okay.
[802.96:813.16] So, I'll give you a reason why neural networks is interesting for me in approximation theory
[813.16:819.0799999999999] perspective and I'll have more motivations for it from the statistical learning theory
[819.0799999999999:822.0] perspective later on.
[822.0:827.9599999999999] It turns out that the way we constructed the neural networks is the activation function
[827.9599999999999:839.04] is has certain properties, meaning that it needs to be an increasing, a non-decreasing
[839.04:847.64] function, it needs to be, you cannot write it in terms of a polynomial, finite polynomial
[847.64:849.92] expansion and so on and so forth.
[849.92:858.24] In this case, let's say given a bounded space, let's say the unit cube, then you define
[858.24:865.24] any function in this unit cube, any continuous function in this unit cube, then you can actually
[865.24:877.44] define some weights that will approximate the function that you chose with some parameters.
[877.44:878.44] All right.
[878.44:887.12] So, here, here's our protein code neurons.
[887.12:890.96] This is the size of the protein code hidden layer.
[890.96:897.6] Right? So, because we observe the input, we observe the output, right?
[897.6:904.5600000000001] But we don't observe the hidden layer, we train the hidden layer and as a number of neurons
[904.5600000000001:912.0] increase, you have more and more parameters that you can use to approximate a function and
[912.0:925.0] it turns out that when the width of the neural network is large enough, you can approximate
[925.0:926.0] any function.
[926.0:930.8] I mean any continuous function and this is an important property.
[930.8:939.04] There's of course a fine print here is that, you know, if you, I mean, with great approximation
[939.04:943.04] power comes great computational burden, right?
[943.04:951.04] Number of neurons and may need to be exponentially large.
[951.04:954.04] Okay.
[954.04:968.04] So, so neural networks, why we're not, maybe not as popular, okay?
[968.04:977.04] Neural networks were the popularity of neural networks just like our neural spike has been
[977.04:978.04] spiking.
[978.04:987.28] And then, you know, Jeff Hinton and Jan Lecon, who have some vikers papers that shows some
[987.28:990.5999999999999] impressive classification results.
[990.6:998.28] And then, you know, there would be people using neural networks to beat Beckham and Tariya
[998.28:1001.88] Sanjouski was doing something like this.
[1001.88:1007.72] So they were somewhat popular, but they're like crazy popular currently.
[1007.72:1013.6] So what's were the reasons for their popularity?
[1013.6:1019.1600000000001] You know, what were the reasons that they were not as popular, all right?
[1019.16:1027.72] So first, if you think about it, you know, if you can optimize, so sorry, if you can approximate
[1027.72:1032.44] any function, these things are crazy useful.
[1032.44:1036.36] You know, you can try to approximate any function.
[1036.36:1041.8] You know, if you think about it, I give you cancer images and the output, say, is cancer
[1041.8:1044.96] or not, there's a functional mapping, right?
[1044.96:1047.6] There's an input in an output.
[1047.6:1053.56] And ideally, these things learn as if a doctor does, you know.
[1053.56:1057.52] There are other things you can do, you know, you can have vision tasks, automated driving
[1057.52:1061.1999999999998] tasks, control tasks.
[1061.1999999999998:1071.04] You can learn distributions, you can generate, you know, even gaining environments using
[1071.04:1079.0] genets with the serial networks because you learn the distributions of, let's say, the
[1079.0:1084.44] urban landscape, things like this.
[1084.44:1092.8799999999999] The issue is that if you think about it from the single hidden layer perspective, there's
[1092.8799999999999:1098.08] so many parameters that you may need to fit, right?
[1098.08:1101.1999999999998] And this means that it is actually just too big to optimize.
[1101.1999999999998:1106.8799999999999] Let's say you have the computation, how do you fit the degrees of freedom?
[1106.8799999999999:1111.36] So you need a lot of training data.
[1111.36:1116.08] And because this thing is nonlinear, as you will see, oftentimes, to be able to learn
[1116.08:1120.6799999999998] our parameters of neural network, you would need to do optimization and this optimization
[1120.6799999999998:1127.08] is non-comics, even for the simplest trivial examples, right?
[1127.08:1131.3999999999999] So suppose we take an activation function, which is just identity, right?
[1131.3999999999999:1136.72] It does nothing, identity.
[1136.72:1151.24] So our hidden layer problem becomes x1, x2 and even in the least square cost, this is
[1151.24:1155.96] non-comics.
[1155.96:1162.6000000000001] So this not only poses computational issues, but you don't benefit from having a local
[1162.6000000000001:1170.64] optimal solution being global with complexity, for instance.
[1170.64:1176.56] But things have been changing since 2010.
[1176.56:1182.64] Our computational power has been increasing like the stock price of NVIDIA.
[1182.64:1185.68] Also AMD may be killing over the last year.
[1185.68:1190.88] I think it increased triple.
[1190.88:1196.8] And as you can see, we now generate significant amounts of data.
[1196.8:1204.6000000000001] So we have datasets, Google managed to collect datasets, medical datasets, text that we can
[1204.6000000000001:1205.6000000000001] use.
[1205.6000000000001:1212.68] So all of a sudden, we have the computation, we have the data, but this is the puzzling
[1212.68:1214.96] part finding the optimum.
[1214.96:1215.96] All right?
[1215.96:1221.96] So hopefully we'll try to explain this a little bit in this and next classes.
[1221.96:1223.2] Okay.
[1223.2:1230.04] So let's think about an application.
[1230.04:1232.88] So multi-class classification.
[1232.88:1239.3600000000001] So remember, we now have enough data.
[1239.3600000000001:1241.3600000000001] We can do computation.
[1241.36:1246.36] Can we actually find the minimum or can we find something that actually makes us find
[1246.36:1253.9199999999998] not necessarily minimal, but maybe use some algorithms to try something and see if it's
[1253.9199999999998:1256.24] actually useful in any task.
[1256.24:1257.52] All right?
[1257.52:1259.52] So here are some examples.
[1259.52:1260.52] There's the image net.
[1260.52:1267.6399999999999] There's the CFR, there's Canadian Institute dataset.
[1267.64:1272.3600000000001] So here you have some 10 classes, you're given an image, you try to guess the class.
[1272.3600000000001:1273.3600000000001] Right?
[1273.3600000000001:1279.3600000000001] So you have some samples, image label pairs, they come from an unknown distribution.
[1279.3600000000001:1282.4] So what you would like to do is find the multi-class function.
[1282.4:1283.4] Right?
[1283.4:1287.1200000000001] So in this case, it's not a single output, it's multi-alput.
[1287.1200000000001:1295.16] And what you would like to do is minimize the classification probability.
[1295.16:1299.96] I mean, this problem used to be an academic problem.
[1299.96:1307.1200000000001] It was of course important for the industry, but when I was doing my PhD problems like this
[1307.1200000000001:1312.88] for all around, they would have these datasets, Pascal challenge datasets and so on and so
[1312.88:1315.24] forth.
[1315.24:1331.44] And for the image net challenge, we have classical models and then came AlexNet.
[1331.44:1336.96] Look at the drop in the misclassification.
[1336.96:1344.92] And then you get, let's say, here LyconNet Google.
[1344.92:1349.88] And this is similar to, like, we're really getting close to the human performance.
[1349.88:1354.0] And then there's ResidentNet, ResidentNetwork.
[1354.0:1357.6000000000001] And then there's others.
[1357.6000000000001:1368.04] So now you have superhuman performance or a task that was thought to be not possible.
[1368.04:1375.84] So somehow these guys are not claiming that they do optimal training, they take the data,
[1375.84:1383.76] they set up a network, they apply some algorithms and they get some performance that beats humans.
[1383.76:1384.76] Yeah?
[1384.76:1396.72] And this is the interesting part that we'll try to maybe demystify a little bit so that
[1396.72:1403.48] we know the strengths and the weaknesses of this particular pattern.
[1403.48:1404.48] Good.
[1404.48:1413.2] Now, if you think about it, what I introduced so far was one hidden layer neural network
[1413.2:1415.68] that has some parameters.
[1415.68:1423.4] And it turns out that you can make your own, there's like a cookbook of neural networks
[1423.4:1427.88] and one important one is this convolutional neural network.
[1427.88:1433.64] What the convolutional neural network does is that it looks at the input and then as opposed
[1433.64:1439.2800000000002] to doing dance matrix multiplications, it does what is called as a convolution.
[1439.2800000000002:1442.24] So the parameters is much less.
[1442.24:1449.1200000000001] But then, you know, you do deeper structures.
[1449.12:1455.12] There's the activation, given the activation layer, you pull, maybe take the maximum within
[1455.12:1457.84] a certain block and so on and so forth.
[1457.84:1462.8] And then in the output, you end up using some of this.
[1462.8:1473.4399999999998] So when we talk about neural networks, we don't just mean that you take a dance with
[1473.4399999999998:1477.08] matrix to do a refined transformation that apply an activation.
[1477.08:1489.8799999999999] There's some creativity here and you can put your own structures that kind of sense.
[1489.8799999999999:1494.28] So why would you want to use say things like promolutions?
[1494.28:1495.28] All right.
[1495.28:1503.3999999999999] This is where, you know, in the function estimate that we're parameterizing some functions.
[1503.4:1507.2800000000002] There's what people call as the inductive bias.
[1507.2800000000002:1513.52] So we kind of know certain structures that work well, like the human visual cortex, like
[1513.52:1518.8000000000002] the occipital region being at the end.
[1518.8000000000002:1524.0400000000002] You know, humans are super quick to respond to visual cues.
[1524.0400000000002:1530.88] In fact, as you go, for example, as the signals transfers towards the occipital region, you
[1530.88:1535.92] already have some sort of an idea of what things are.
[1535.92:1543.48] And what the human visual cortex does is, you know, what people argue are these like convolutions
[1543.48:1546.2800000000002] and sparse filtering.
[1546.2800000000002:1549.5200000000002] Okay.
[1549.5200000000002:1557.44] So here's a bit of an explanation why the architecture of the neural network is important
[1557.44:1563.3600000000001] because it induces this inductive bias in the function class.
[1563.3600000000001:1564.3600000000001] Okay.
[1564.3600000000001:1575.28] So, so suppose this is the, these are function class that you're trying to find functions in.
[1575.28:1578.24] It's a large class.
[1578.24:1581.8400000000001] And let's say there exists a true unknown function.
[1581.8400000000001:1583.0800000000002] You know.
[1583.08:1588.8] Now, think about fixing the number of parameters to P.
[1588.8:1591.56] So this is our optimization dimension.
[1591.56:1595.4399999999998] So X element of Rp.
[1595.4399999999998:1597.76] Now imagines little X.
[1597.76:1603.56] Now has bunch of matrices and so on and so forth.
[1603.56:1609.72] And let's say the total number of entries here is P.
[1609.72:1610.72] Okay.
[1610.72:1621.44] Now, you can imagine having a fully dense network with just P parameters.
[1621.44:1629.88] So if it was a hidden, just a single hidden layer, you can have, you know, some dense matrix,
[1629.88:1637.08] activation fuck up, activation function, another vector multiplying it.
[1637.08:1641.6799999999998] But suppose you use convolutional network that I just mentioned.
[1641.6799999999998:1650.56] Remember, convolution is, so you can write convolution as the matrix that maps, let's say, some
[1650.56:1655.84] N dimensions to N dimensions, but it does not have N squared degrees of freedom.
[1655.84:1659.8799999999999] It only has N degrees of freedom when you do convolution.
[1659.8799999999999:1661.8799999999999] All right.
[1661.88:1668.2800000000002] In this particular case, you can imagine deeper networks that you can create with P parameters
[1668.2800000000002:1671.0800000000002] and use convolutions.
[1671.0800000000002:1677.3200000000002] So this class of architectures that can be created by convolutions intersect with the
[1677.3200000000002:1681.96] fully dense ones.
[1681.96:1688.8400000000001] The fully dense ones can be separate and the convolutional neural networks can be separate.
[1688.84:1694.8799999999999] And by creating these convolutional architectures, the convolutional layers, activation function,
[1694.8799999999999:1705.28] pooling, and again, the convolutional layers, you create a function class that may be closer
[1705.28:1711.84] to the true function of interest than a fully dense one with P parameters.
[1711.84:1712.84] All right.
[1712.84:1721.12] So, ideally, if you increase P, this thing can encapsulate this convolutional layers as a
[1721.12:1722.12] special case.
[1722.12:1723.12] All right.
[1723.12:1731.1999999999998] But it may require maybe way more parameters than necessary.
[1731.1999999999998:1736.8] So by keeping number of parameters down by using the architecture, you can create function
[1736.8:1743.48] classes that uses our prime knowledge on what the function should be.
[1743.48:1744.48] OK.
[1744.48:1748.6399999999999] And if you remember the philosophy example, this is a very similar story.
[1748.6399999999999:1757.08] You try to use your prime knowledge to seek function estimates that have certain properties.
[1757.08:1761.8] That way you bias with number of parameters that you use.
[1761.8:1767.08] You allow these functions to be closer to the, you know, code and code hypothetically
[1767.08:1770.24] true unknown function of the supervisor.
[1770.24:1771.24] All right.
[1771.24:1772.24] Does this make sense?
[1772.24:1776.24] Is it true early in the morning?
[1776.24:1777.24] Any questions?
[1777.24:1778.24] All right.
[1778.24:1779.24] Good.
[1779.24:1796.64] Now, this does not prevent us from using more and more parameters.
[1796.64:1804.88] So I think that the generative pre-trained transformer network has been on the news lately
[1804.88:1809.5200000000002] or the last month, quite a bit.
[1809.5200000000002:1815.92] I think it had 170 billion parameters or something like this.
[1815.92:1821.5200000000002] And I don't know if you've seen the text that generates the answers that gives.
[1821.5200000000002:1825.72] You can start a paragraph and then it feels like an essay for you.
[1825.72:1828.7600000000002] You can ask math questions and it answers them.
[1828.7600000000002:1832.5200000000002] It's very impressive.
[1832.52:1840.84] And so you create a structure that has the transformer architectures which we will
[1840.84:1846.28] talk about in the recitation a little bit this, this Friday.
[1846.28:1851.04] You use your inductive biases and in the end you create these function estimators that
[1851.04:1854.28] taking inputs and then generating impressive outputs.
[1854.28:1855.28] All right.
[1855.28:1862.96] So of course, the more parameters, the harder the problem is and remember, you set up
[1862.96:1865.24] optimization problems that are non-comics.
[1865.24:1872.96] So somehow there's stationary points, there's first sort of stationary points, there's
[1872.96:1875.96] saddle points.
[1875.96:1884.24] So how do we succeed despite all these challenges of non-comics?
[1884.24:1893.08] So let's talk about this a little bit more so that it's a bit more explicit.
[1893.08:1897.08] So for this we're going to recall the empirical risk minimization.
[1897.08:1911.08] If you recall the way we...
[1911.08:1919.1599999999999] So our generator was generating these AI, our supervisor was giving us EIs and with the
[1919.1599999999999:1928.8] learning machine we talked about the parametric estimation model that we were trying to predict
[1928.8:1931.9199999999998] these labels.
[1931.92:1943.5600000000002] And for this initially we talked about minimizing, so think of this maybe learner, we thought about
[1943.5600000000002:1948.6000000000001] minimizing a loss between our prediction and the data points, right?
[1948.6000000000001:1954.5600000000002] At least our function to succeed in the training data.
[1954.56:1966.32] Now at this point we talked about for example minimizing the population risk, right?
[1966.32:1970.6799999999998] But we didn't know the distribution for the data, the samples.
[1970.6799999999998:1975.76] So instead we approximated this with the empirical risk meaning the average, right?
[1975.76:1982.1599999999999] The strong law of large numbers help us in quantifying how close this empirical risk
[1982.16:1985.1200000000001] is to the population risk, right?
[1985.1200000000001:1990.5600000000002] The true risk is n-grows, v-dv-h-less and less from it, right?
[1990.5600000000002:1994.8400000000001] That was the blessing of dimensionality.
[1994.8400000000001:1999.4] And in this particular case there are a bunch of loss functions that we use, logistic
[1999.4:2007.28] loss, square loss, hinge loss that we talked about, you know, pseudometrics, metrics, things
[2007.28:2011.96] like this.
[2011.96:2023.6000000000001] Now if you think about plugging in a neural network into the empirical risk minimization problem,
[2023.6000000000001:2028.28] so we are trying to minimize, we just fit the parameters.
[2028.28:2037.32] We have one over n some loss function parameters.
[2037.32:2042.08] So we think about minimizing this, right?
[2042.08:2049.7599999999998] This is no longer the case if you go with neural networks here.
[2049.7599999999998:2055.64] Just because of the presence of activation function or the multiple layers, you deviate
[2055.64:2059.08] from the linear models, logistic models and so on and so forth.
[2059.08:2067.4] And we end up getting, let's say, optimization landscapes that look more like this and like
[2067.4:2069.4] this.
[2069.4:2078.68] So this is typically what academia likes, right?
[2078.68:2087.88] And this is typically real life looks like maybe the actual global minimums somewhere here,
[2087.88:2092.36] but if you were to try to do gradient descent, you may basically get stuck already here,
[2092.36:2099.76] which may be very far away from where you want to end up.
[2099.76:2106.2400000000002] And because of this particular challenge, the conventional wisdom, right up until 2010,
[2106.2400000000002:2112.44] is that we use simple models, we use simple error functions that we could characterize.
[2112.44:2119.44] And you will see such a characterization in the second half of this lecture.
[2119.44:2123.64] All right.
[2123.64:2129.48] So here are some of the interesting elements, right?
[2129.48:2136.44] Now we have master datasets, so this is a check.
[2136.44:2145.84] We now have enough understanding of some of these parametric forms of the functions.
[2145.84:2157.0] You know, you can create these inductive biases in the form, how you set up the function.
[2157.0:2165.56] And then you end up coming to a landscape seemingly like this.
[2165.56:2171.2] And you use some optimization methods such as the stochastic gradient descent and some
[2171.2:2174.2] magical things happen.
[2174.2:2176.4] All right.
[2176.4:2179.4] Okay.
[2179.4:2185.7999999999997] At this point, I would like to tell you a little bit about some of the key challenges before
[2185.7999999999997:2191.48] delving into the theory for it.
[2191.48:2194.84] So that, you know, we're clear.
[2194.84:2201.6000000000004] What I've done so far is to give you some examples where deep learning has impressed
[2201.6000000000004:2204.04] its superhuman performance.
[2204.04:2210.08] I only showed you some of the vision tasks.
[2210.08:2218.04] There are other tests, control tasks that are also very, very promising, sub-driving
[2218.04:2223.92] cars where you use deep learning architectures.
[2223.92:2234.7200000000003] It turns out that as successful as these things are, that they're very setting dependent.
[2234.7200000000003:2247.76] So you can create tiny perturbations and the input that will completely change the classification.
[2247.76:2254.36] And some of the most striking examples I have to hear is this one.
[2254.36:2255.36] All right.
[2255.36:2259.5200000000004] So here, subturdle, maybe a tortoise.
[2259.5200000000004:2265.5200000000004] I think it's a turtle because it swims.
[2265.5200000000004:2269.8] As you can see, for a human, this is clearly a turtle or a tortoise.
[2269.8:2272.1200000000003] It cannot be expressed.
[2272.12:2281.4] But by adding imperceptible perturbations to this image, you can make a state of the
[2281.4:2287.44] art classifier, classify the turtle as a rifle.
[2287.44:2292.0] And here's another demonstration.
[2292.0:2296.92] I think it was the test engineer that did this or something like this.
[2296.92:2299.44] That you take a stop sign.
[2299.44:2304.96] You put some stickers on top.
[2304.96:2312.04] And all of a sudden, you're classifier that recognizes the stop sign.
[2312.04:2318.0] It classifies a bit of 45 miles per hour speeds on.
[2318.0:2319.0] Right?
[2319.0:2329.2400000000002] And this is like 80 kilometers per hour speed, which is terrible.
[2329.24:2342.2] And there are reasons for this particular case, which I would like to explain here.
[2342.2:2347.08] So I'll take the simplest classification example.
[2347.08:2352.04] So let's say you have a unit pole.
[2352.04:2359.2] And let's say you have the simplest example where the equator is our decision boundary.
[2359.2:2365.12] And you have dogs as data set here, uniformly distributed.
[2365.12:2373.3599999999997] And you have cats here, uniformly distributed.
[2373.3599999999997:2380.9199999999996] And our neural network will be basically approximating this particular equator as the
[2380.9199999999996:2382.3999999999996] now.
[2382.4:2389.12] So by perturbations, I mean the following, so you take the neural network, parameterized
[2389.12:2392.6800000000003] by our parameters, you have an input, you add a tiny perturbation.
[2392.6800000000003:2393.6800000000003] All right?
[2393.6800000000003:2394.6800000000003] Small.
[2394.6800000000003:2398.6800000000003] So that would be cool.
[2398.6800000000003:2411.36] I mean, usually that would equate to taking a slice here of the equator.
[2411.36:2412.52] Right?
[2412.52:2416.88] And then by, so maybe this could be epsilon two epsilon.
[2416.88:2417.88] Right?
[2417.88:2424.1600000000003] Anything here, by taking an epsilon perturbation, you can switch to the decision.
[2424.1600000000003:2427.6800000000003] This is easy to see.
[2427.6800000000003:2436.8] So if you take this particular example, so let's say we have a nice radius one, you know,
[2436.8:2440.52] the say our description boundary is exactly in the middle.
[2440.52:2445.88] You take an epsilon slice on the left, epsilon slice on the right, then anything in between
[2445.88:2451.44] by coming up with the proper epsilon perturbation, you can switch to the classification boundary.
[2451.44:2458.28] Now, in this case, the amount of decisions that you can change is somewhat limited.
[2458.28:2459.28] Right?
[2459.28:2465.88] So if you think about it, there's so much more volume here or area here than in this particular
[2465.88:2468.16] tiny looking area.
[2468.16:2471.24] So this is 2D.
[2471.24:2473.24] Okay?
[2473.24:2477.8799999999997] It turns out that, you know, so like this means that, you know, at the state of perturbations
[2477.8799999999997:2483.0] actually can maybe perturb some fraction of the data.
[2483.0:2489.3599999999997] Because remember, you pick, for example, a data point and somebody adds a tiny perturbation
[2489.3599999999997:2494.48] knowing your structure or knowing the problem.
[2494.48:2501.32] And the decision, your neural network will have the wrong decision, for example.
[2501.32:2503.8] And it looks like the damage is limited.
[2503.8:2510.52] It's limited to some small area that you don't necessarily need to worry about this.
[2510.52:2517.32] Unfortunately, this particular understanding completely breaks down in higher dimensions,
[2517.32:2519.0] in the following sense.
[2519.0:2520.0] Right?
[2520.0:2525.8] Did you start to increase the dimensions, so let's say you're looking at 100 dimensional
[2525.8:2529.32] cube, sorry, unit ball.
[2529.32:2530.32] Okay?
[2530.32:2531.32] Right?
[2531.32:2537.28] So it has hundreds of dimensions and you again do this X1 slice.
[2537.28:2538.28] Yeah?
[2538.28:2543.24] Now, here's the funny thing.
[2543.24:2552.8399999999997] To be able to get to, let's say, more than 95% of the volume, that epsilon now becomes
[2552.8399999999997:2555.3599999999997] 0.2.
[2555.3599999999997:2562.68] And if you go to, you know, 1000 or millions of dimensions, then that epsilon becomes vanishing
[2562.68:2564.08] you small, actually.
[2564.08:2565.08] Right?
[2565.08:2571.56] Meaning in high dimensions, most of the volume actually lives in that tiny slice, this
[2571.56:2574.08] epsilon slice.
[2574.08:2580.08] Now this is known as the blisting of dimensionality.
[2580.08:2583.2799999999997] And in this particular case, it is actually hurting you.
[2583.2799999999997:2587.2] This is precisely the concept of concentration.
[2587.2:2592.56] So concentration of measures and so on and so forth, like deviations from the expected
[2592.56:2597.4] value becomes harder in dimensions than this one, the most.
[2597.4:2598.4] Right?
[2598.4:2605.88] And in this particular case, you take uniformly distributed data in a sphere, take the equator
[2605.88:2611.44] as a decision boundary, take epsilon slices left and right, and then all of the terms
[2611.44:2620.64] in high dimensions, you realize that most of the data is actually living in that disk.
[2620.64:2627.96] Now a simple approximation, like Taylor series expansion, shows you.
[2627.96:2644.2] That this must be related to what the gradient of the neural network, how fast it is changing
[2644.2:2645.2] is the culprit.
[2645.2:2646.2] Right?
[2646.2:2651.48] Because if you think about it, your decisions, their differences are somewhat bounded by
[2651.48:2658.48] the perturbation, which is small, but neural networks in all of the approximates may have
[2658.48:2662.56] rapidly changing regions.
[2662.56:2663.56] Right?
[2663.56:2669.64] And hence the lift shift constant of the neural network is an important property.
[2669.64:2670.64] Okay?
[2670.64:2671.64] All right.
[2671.64:2679.08] This is actually a very active research area.
[2679.08:2683.4] And we take maybe a 15 minute break and continue more often.
[2683.4:2711.92] Thank you.
