~Lecture 7 (2021)
~2021-11-01T12:04:22.713+01:00
~https://tube.switch.ch/videos/TUkuUBm0wJ
~EE-556 Mathematics of data: from theory to computation
[0.0:2.0] All right.
[6.0:8.0] That's something.
[8.0:12.0] Welcome back to Tuesday, January 9.
[12.0:17.0] Sorry, it's a little time.
[17.0:21.0] This means I can maybe even use the old jokes again.
[21.0:27.0] The video is called in dark, just like my point.
[27.0:30.0] So what we're going to do today is we're going to talk about
[30.0:31.0] deep learning.
[31.0:34.0] I think the people who are now
[34.0:40.0] doing machine learning, they do it so that they can say they know
[40.0:41.0] deep learning.
[41.0:44.0] Just a little bit of top jobs on.
[44.0:48.0] This is almost a religion now.
[48.0:51.0] Right. You know, five papers on top of each other that can
[51.0:52.0] stick each other.
[52.0:55.0] Nobody cares because it is so clear.
[55.0:60.0] So what we're going to do is take a think of the twist so that
[60.0:65.0] between maybe understand a little bit.
[65.0:72.0] I think this word that is kind of through the BS from.
[72.0:75.0] I apologize for my voice.
[75.0:79.0] I thought it was a good idea before the rain season started.
[79.0:84.0] I could literally cut a lot of bushes in my garden.
[84.0:87.0] I thought I could.
[87.0:90.0] I don't like all the bushes.
[90.0:93.0] All right.
[93.0:95.0] So.
[95.0:99.0] What we're going to do is we're going to ease ourselves in deep learning.
[99.0:104.0] So what I will do is I'm going to do the motivation for.
[104.0:105.0] The same.
[105.0:106.0] Network models.
[106.0:110.0] And then I'll give you some of some of the made with some of the
[110.0:114.0] different things that I'm going to do.
[114.0:116.0] I'm going to do it.
[116.0:118.0] It may look like.
[118.0:120.0] But the idea is actually you better get them.
[120.0:122.0] And then we're going to do it.
[122.0:124.0] The lines of deep running.
[124.0:127.0] So we're going to talk about what is called as the market.
[127.0:128.0] The fundamental.
[128.0:129.0] Now this is Friday.
[129.0:134.0] There will be an interview of learning recitation.
[134.0:141.0] It's a presentation.
[141.0:144.0] It's a few.
[144.0:147.0] It's a few.
[147.0:155.0] We're going to talk about deep learning.
[155.0:163.0] Maybe some of the subtle.
[163.0:166.0] And generalization.
[166.0:168.0] Things like this.
[168.0:173.0] I think that anybody who wants to become an engineer and sign close.
[173.0:176.0] What do you want to put.
[176.0:179.0] You want to make a.
[179.0:180.0] You want to make a.
[180.0:182.0] You want to make a more salary.
[182.0:188.0] It's from the mentors, right.
[188.0:190.0] So.
[190.0:193.0] I don't know if you notice now.
[193.0:196.0] I've been using a particular patient.
[196.0:198.0] There are many notations.
[198.0:200.0] But these forms were ours.
[200.0:205.0] And if you actually look at some of the papers in the deep learning literature,
[205.0:207.0] you will see.
[207.0:209.0] I'm here.
[209.0:211.0] I'm telling you what the map anyways.
[211.0:213.0] So for us, the data.
[213.0:216.0] The sample has been.
[216.0:221.0] So.
[221.0:222.0] We want to make a.
[222.0:223.0] The.
[223.0:224.0] The.
[224.0:225.0] The.
[225.0:226.0] The.
[226.0:228.0] The.
[228.0:229.0] For us.
[229.0:230.0] Optimizers.
[230.0:232.0] Ex is the organization.
[232.0:233.0] They're both.
[233.0:234.0] We've run weights.
[234.0:237.0] So if you think about it.
[237.0:241.0] We'll be again using our notation of decision valuable.
[241.0:246.0] Where is in the deep learning literature, this is called.
[246.0:247.0] It's just.
[247.0:249.0] I know.
[249.0:251.0] It's there once more.
[251.0:253.0] That it's not like a language.
[253.0:257.0] Maybe like Spanish.
[257.0:259.0] It's not exactly the same.
[259.0:261.0] But you can try to infer.
[261.0:264.0] There will be bias.
[264.0:266.0] So for us, the labels are.
[266.0:271.0] The.
[271.0:272.0] The.
[272.0:273.0] The.
[273.0:274.0] The.
[274.0:275.0] The.
[275.0:280.0] And then the labels, which we were using as the.
[280.0:282.0] Old and why.
[282.0:285.0] So just.
[285.0:289.0] Be careful about this particular location change.
[289.0:293.0] I want to raise the awareness.
[293.0:296.0] It's the.
[296.0:297.0] The.
[297.0:298.0] The.
[298.0:299.0] The.
[299.0:300.0] The.
[300.0:301.0] The.
[301.0:302.0] The.
[302.0:303.0] And then you can see here.
[303.0:304.0] And then you can see here.
[304.0:306.0] Because you see.
[306.0:307.0] So the slides you see.
[307.0:310.0] If you're not thinking about it, and you're looking at the paper.
[310.0:312.0] It means you can use it.
[312.0:313.0] I just can be.
[313.0:315.0] I just can be.
[315.0:316.0] Alright.
[316.0:317.0] Now.
[317.0:319.0] I mean certain decorations.
[319.0:320.0] I'm not.
[320.0:321.0] Right.
[321.0:326.4] And so things like it are even Google uses the email as a specific and and
[330.12:339.16] That's the time it's quite to and so let's take a particular vinyl test patient problem where we have some data set, you know, so here
[340.12:348.28] Processed or circles and if you remember about logistic regression, I think you've seen this either in the pen mark or one of the pen boxes that
[348.28:350.28] I
[350.28:352.28] Which is the condition the standard
[353.79999999999995:355.79999999999995] Logically
[356.2:358.2] You know we had this particular code
[358.76:368.0] We were able to write down some sort of a post function with just the relation post country and we were able to talk about how to learn
[368.4:371.4] Basically parameters which finds a separating paper plant
[371.4:375.56] So you could actually in this particular case
[379.0:382.44] Put a decision boundary that must be separate
[383.23999999999995:385.23999999999995] in the classes
[385.24:396.76] Beautiful wow, this is very good for a fixed example and the reality could be a bit different. So use the data set
[399.16:401.16] And
[401.40000000000003:405.56] Those believe the ball or fly is the question any remarks
[405.56:410.62] The
[413.48:415.48] You talked about
[418.12:420.6] thinking about this
[422.52:424.52] Anioni
[426.84000000000003:427.34000000000003] want
[427.34:433.73999999999995] Generally, you want to particularly put the separate pact when we want to use a separate
[433.73999999999995:436.94] pact because it is not being separable.
[436.94:437.94] Right?
[437.94:441.82] That is a challenge.
[441.82:454.21999999999997] But the canonical shape, the initial right, has been used, you know, like this list
[454.22:459.54] in the data may not be being separable in the ambient thing.
[459.54:464.70000000000005] But what we think is different features are not in sort of a kind of a kind of meeting
[464.70000000000005:466.66] in the sector of it.
[466.66:473.46000000000004] And that involved this whole thing, well, to keep all through a thing.
[473.46000000000004:479.46000000000004] And here, if you want to say, which is like a particular example that makes it very easy
[479.46000000000004:480.46000000000004] to be realized.
[480.46:494.21999999999997] So, the idea is that you can take this particular set, the parameter, which is the say, the
[494.21999999999997:495.7] distance from the origin.
[495.7:506.09999999999997] So, as to this is the origin, in that particular case, you can see that under this particular
[506.1:513.7] mapping, distance from the origin, the data somehow becomes really separate.
[513.7:523.7] So, this means that the hammer that we shine, you know, prepared, still applies to this
[523.7:539.0200000000001] kind of transformation.
[539.0200000000001:546.1400000000001] Of course, issue with this thing is that we have to build a big dimension, which is arguably
[546.1400000000001:551.7] larger this series, I 50% larger, but maybe there's a problem with it, but you need much
[551.7:556.9000000000001] more of that.
[556.9000000000001:563.1] And this kind of underlines the kernel methods, the random features models, and all kinds
[563.1:564.1] of stuff.
[564.1:571.6600000000001] The data, the create and the features out of it, so the random features, it becomes
[571.6600000000001:573.6600000000001] the unit sector.
[573.6600000000001:580.6600000000001] You know, if people understand this, you know, nothing has changed about this two concentric
[580.66:584.26] circles or this is whatever.
[584.26:589.66] It's just created an inclusion of the check.
[589.66:594.66] Now, as opposed to two dimensions, we look at data and three dimensions, and also so
[594.66:598.9] we can't use it exactly.
[598.9:609.66] Now, what's an alternative to this pattern?
[609.66:616.66] Because this was, this is getting some job done, but apparently not good enough.
[616.66:624.66] So, what I'm going to do is I'm going to introduce the new renet first.
[624.66:627.66] It's the first that you saw be chanted.
[627.66:634.66] We'll start with a one new renet for, we're going to have new ones.
[634.66:641.66] The first of all, I, I would recommend that you either be turning paper, do not trust
[641.66:649.66] any word you see, you know, in the sense that the neuron is not necessarily a neuron.
[649.66:654.66] Neurons, they can be fires, fights, fear, there's nothing's fighting and so forth.
[654.66:660.66] Deep learning as this connotation that we have intelligent machines, it's not just a function
[660.66:662.66] and nothing you will see.
[662.66:672.66] So, maybe my sense is contagious and you will be a bit cynical when you're reading these things.
[672.66:677.66] So try to pay attention to the metacic constructs and not necessarily the words,
[677.66:682.66] which are big mouth convoyant and attention is only needed.
[682.66:690.66] This particular field where, when you think of yourself the words like the discriminator is going to fall to.
[690.66:697.66] I will explain what this thing matter is.
[697.66:703.66] Okay, so we're going to talk about a function.
[703.66:708.66] The case if you remember the techniques, particular generators,
[708.66:721.66] particularizer, the learning machine, we have a, b and then you have this phonetic model, h, h, h of a, we both about having far enough follows and if you do anything,
[721.66:727.66] you say, you said, you know, no, no, if you were to perform on the professions, you take the country class,
[727.66:730.66] x would be a phenomenon of the professions and you have a function.
[730.66:735.66] Now, what we're going to do is we're going to use something else.
[735.66:740.66] Hold the neural network.
[740.66:744.66] The idea being that you have an input.
[744.66:748.66] We're going to have some parameters.
[748.66:750.66] We're going to have some biases.
[750.66:755.66] And we're going to have what is called as an activation.
[755.66:765.66] What we're going to do is we're going to take input and apply an antontransformation kind of like what we do in the thing.
[765.66:780.66] If you're thinking about random features, what we will do is we will put x1 into some random dimensionality of using numbers.
[780.66:787.66] And then what we're going to do is we're going to make this task to a non-linear.
[787.66:792.66] This is all the activation function.
[792.66:796.66] Is there anything magic?
[796.66:807.66] If I may sound like I'm trying to figure out why I'm using the first.
[807.66:811.66] Then you follow it works in other.
[811.66:823.66] FI and transformation.
[823.66:826.66] You learn how to take the river.
[826.66:830.66] It is pull it back to off.
[830.66:838.66] That was a big thing.
[838.66:840.66] I apologize.
[840.66:848.66] I think that the impact is.
[848.66:854.66] So the point about this is that in the end.
[854.66:863.66] The point where our problem is.
[863.66:867.66] The whole thing is the way it makes the cease.
[867.66:869.66] The multi-optic.
[869.66:871.66] The network.
[871.66:873.66] The screen is not necessarily effective.
[873.66:875.66] But it's a matrix.
[875.66:877.66] You have the activation function.
[877.66:879.66] You have your bias terms.
[879.66:884.66] And I'm going to get some examples.
[884.66:885.66] Big picture.
[885.66:887.66] Little things that would be our decision.
[887.66:888.66] I vote.
[888.66:890.66] And wall.
[890.66:892.66] And.
[892.66:896.66] Or think about this.
[896.66:899.66] You want to make this a multi-linear network.
[899.66:901.66] You just need to see the.
[901.66:903.66] Wings and repeat.
[903.66:906.66] Put the another activation function.
[906.66:916.66] This is the most basic.
[916.66:920.66] And we continue to.
[920.66:927.66] I'll tell you a little bit more about the other architectures.
[927.66:937.66] And we're going to talk about the.
[937.66:940.66] The thing.
[940.66:941.66] The thing.
[941.66:942.66] The.
[942.66:943.66] The.
[943.66:944.66] The.
[944.66:945.66] The.
[945.66:946.66] The.
[946.66:947.66] The.
[947.66:948.66] The.
[948.66:949.66] The.
[949.66:950.66] The.
[950.66:951.66] The.
[951.66:952.66] The.
[952.66:953.66] The.
[953.66:954.66] The.
[954.66:955.66] The.
[955.66:958.66] The.
[958.66:963.66] Is that.
[963.66:964.66] Special action sensor.
[964.66:968.66] This unsere direction pattern actually.
[968.66:974.66] I'm sure some of theBaby ein or shouldn't.
[974.66:975.66] Anything.
[975.66:977.66] The software range.
[977.66:980.66] If I want to.
[980.66:981.66] The software.
[981.66:982.66] To thewhy.
[982.66:983.66] And.
[983.66:987.66] that the unfortunate if you can any fix the season.
[989.66:995.66] Think about it. Any function. Any continues.
[1001.66:1005.66] We imagine taking the image and providing for the one
[1005.66:1009.66] out there is this cancer of taking the image, thinking about
[1009.66:1013.66] what there are. There's a fact in that it's a function.
[1013.66:1017.66] Let's assume this is a continuous function.
[1019.66:1025.6599999999999] Illeges go within 0.25 depending on whatever the conversation needs.
[1025.6599999999999:1029.6599999999999] So they move on some sort of a few.
[1031.6599999999999:1033.6599999999999] So.
[1033.6599999999999:1037.6599999999999] Let's assume there is this function. I'm not saying that there exists a
[1037.66:1041.66] function.
[1041.66:1045.66] Let's assume that the thing that we were interested in my images.
[1045.66:1047.66] There was a function.
[1047.66:1049.66] How do you plan to find that function?
[1049.66:1053.66] The problem is maybe.
[1053.66:1055.66] Squires.
[1055.66:1057.66] So.
[1057.66:1061.66] The retro.
[1061.66:1065.66] Is a pharmacist form.
[1065.66:1067.66] Is this a specific procedure?
[1067.66:1069.66] You can write.
[1069.66:1071.66] Is this the sense?
[1071.66:1073.66] Is this the motivation?
[1073.66:1077.66] Yeah. Of course, the end.
[1077.66:1081.66] User license agreement.
[1081.66:1083.66] You have to.
[1083.66:1087.66] It turns out that you're able to achieve such a thing.
[1087.66:1089.66] That's a.
[1089.66:1103.66] The number of neurons or the lifting the hexagonal risks can actually.
[1103.66:1107.66] So the very first FI transformation.
[1107.66:1113.66] With.
[1113.66:1127.66] So.
[1127.66:1129.66] Well.
[1129.66:1131.66] Why was it that you know,
[1131.66:1135.66] you look at David's thoughts machine learning protein.
[1135.66:1139.66] You look at some of the patients from the elders of machine learning.
[1139.66:1143.66] They were like on the.
[1143.66:1145.66] Three years of machine learning.
[1145.66:1154.66] It's like, you know, they were station people type of new networks and all the other ones were doing some network.
[1154.66:1156.66] The station.
[1156.66:1157.66] Stabilization.
[1157.66:1159.66] What was it that.
[1159.66:1162.66] I'm preventive from new networks.
[1162.66:1167.66] The impact that they have.
[1167.66:1169.66] Well,
[1169.66:1171.66] for approximating any function.
[1171.66:1175.66] Remember, you know, give them neurons.
[1175.66:1177.66] You watch.
[1177.66:1179.66] What does that mean?
[1179.66:1185.66] Well, if you wanted to do optimization that 19 19 technology.
[1185.66:1187.66] Maybe you could wait attack.
[1187.66:1191.66] And I don't know if anybody is going to be there.
[1191.66:1197.66] So with DJ machines and all that powerful whatever and pure 100.
[1197.66:1201.66] And the GPU is still different.
[1201.66:1205.66] All right.
[1205.66:1208.66] All right.
[1208.66:1210.66] Suppose you did have.
[1210.66:1212.66] The secret at the time.
[1212.66:1216.66] And the data sets were endless size.
[1216.66:1221.66] And the secret of the pit.
[1221.66:1224.66] The real is the multi-painting.
[1224.66:1225.66] So.
[1225.66:1229.66] One of the reasons why I think we translate success.
[1229.66:1234.66] It's a lot of data.
[1234.66:1235.66] And then.
[1235.66:1244.66] To think about it, I said up the problem, you know, did you mention the realization that ensuing is not X.
[1244.66:1263.66] But all in fact, that needs the initial guidance to question violence.
[1263.66:1269.66] You know about I think we wanted to talk about September Eastern.
[1269.66:1274.66] approximation using sophisticated percent, we're going to get into that.
[1274.66:1281.66] If people were not using sophisticated percent, right?
[1281.66:1291.66] And you could not find often all the problems by just the algorithm, you know.
[1291.66:1301.66] And then what happened, you know, what happened is that it's my status thing that I'm going to buy this stock.
[1301.66:1305.66] There are computational powers started increasing.
[1305.66:1308.66] In fact, no, I see an idea stuck here.
[1308.66:1314.66] I really, really, have not found the stock.
[1314.66:1320.66] I must own a test.
[1320.66:1329.66] And then this is maybe an I and send it to Zeta by the amount of data.
[1329.66:1333.66] So all of the certain.
[1333.66:1337.66] I think around 2010, you were the precipice of this.
[1337.66:1344.66] That is, you know, the education was booming the data sets were booming.
[1344.66:1354.66] It came together and found the covers on which the new networks kind of like sits now.
[1354.66:1363.66] The thing about it, you know, you have endless data sets.
[1363.66:1377.66] And then this Canadian, whatever, Steve far data sets, you started having bigger data sets.
[1377.66:1384.66] And people started showing all kinds of mice classification.
[1384.66:1391.66] In fact, I can tell you that computer vision.
[1391.66:1395.66] Computer vision field is extremely difficult.
[1395.66:1398.66] And it's extremely difficult.
[1398.66:1403.66] It starts single processing because there is.
[1403.66:1407.66] And my former post I could tell is a room at Chirapha.
[1407.66:1408.66] It was about nine.
[1408.66:1416.66] The vision person always is to say that it's like single processing, but better, you know, like with more dimensions.
[1416.66:1421.66] People manage to show incredible classification.
[1421.66:1428.66] You know, so you look at, for example, some image label pairs from some animal distribution.
[1428.66:1439.66] You try to run a function or pain minimum list classification probability.
[1439.66:1447.66] And then you start with the definition.
[1447.66:1455.66] We had, I don't know how the images to do this classification, but all of a sudden we started going from 60,000 to 14 billion.
[1455.66:1456.66] Yeah.
[1456.66:1472.66] Children.
[1472.66:1473.66] Yeah.
[1473.66:1480.66] UM.
[1480.66:1485.66] labs, not long, more than one or two days, there's a bunch of new professors,
[1485.66:1487.66] well, it's a big big.
[1487.66:1492.66] All right.
[1492.66:1497.66] And hands, keep learning became popular again starting in 2010.
[1497.66:1505.66] I remember seeing Yellowstone at an information theory workshop.
[1505.66:1512.66] I was talking about using something called convolutional networks, which I was trying to erase it.
[1512.66:1517.66] And take a look at the years.
[1517.66:1526.66] So here are the error rates for, I don't know, the image and that challenge.
[1526.66:1542.66] 11, 12, 13, 14, 15, 16, and around 15, we have passed or surpassed even regular performance.
[1542.66:1546.66] Interesting, no.
[1546.66:1556.66] By human, I mean one person under a topography, who particularly set down and labeled, can check his error rate.
[1556.66:1566.66] And now is leading the division AI division in Kesta.
[1566.66:1573.66] Is this the best humans can do?
[1573.66:1576.66] Maybe this is probably the best.
[1576.66:1587.66] And so it's human being that can do within the language that he has, but maybe not our finest performance versus the machines.
[1587.66:1598.66] So one has to be careful when we things like computers surpass human performance in 2015.
[1598.66:1605.66] It's nice sounds super cool, but remember is from person doing just a blatantly pass.
[1605.66:1626.66] And now this gets always blown out of proportion, the computers are going to take over and is going to have a sign up.
[1626.66:1636.66] So here's some enablers.
[1636.66:1646.66] Volucional neural networks, if you want to learn what a convolutional is, I suggest you only take the signal processing force.
[1646.66:1659.66] And there's one black swan here.
[1659.66:1679.66] The solution is all super processing.
[1679.66:1692.66]ackly, or ability to put some amount of data in the case of determine the extent of delivering to transmission.
[1692.66:1697.66] That's a point right?
[1697.66:1708.66] The reason why convolution was an important decision-making Face in phone or what is called the situation so it's really difficult to take it.
[1708.66:1710.46] So think about it this way.
[1710.46:1716.0] Suppose you get some audio signal and you'd like to maybe get a little something
[1716.0:1720.66] taking sound or some high-pitched frequency noise.
[1720.66:1721.8600000000001] What do you do?
[1721.8600000000001:1722.8600000000001] You just think.
[1722.8600000000001:1726.9] Suppose you would like to communicate over a channel.
[1726.9:1735.26] And there's, you know, your Wi-Fi talks, your smartphone talks, how do you use messages
[1735.26:1736.26] very fast?
[1736.26:1739.66] You filter certain channels, how do you do that?
[1739.66:1741.66] Why can we use things?
[1741.66:1746.56] Something that has worked before, it works.
[1746.56:1752.26] And now what proves I do is to do convolutions and to be used.
[1752.26:1758.46] Filter, local filters, convolutions.
[1758.46:1766.16] And then you have some non-linearities, activations, cooling, I, which is literally something
[1766.16:1767.16] like a sub-something.
[1767.16:1773.16] You can put that up a few minutes.
[1773.16:1777.5600000000002] And what is interesting about this is that the center, the filter can have a new network
[1777.5600000000002:1782.16] so you could use a freedom and this particular app which is much less.
[1782.16:1790.66] Meaning, the structure with any, not only that, the optimization is a bit easier because
[1790.66:1794.16] there's less power and there's less connectionness.
[1794.16:1796.16] Right?
[1796.16:1801.16] I think you did that because you didn't have the notification in power.
[1801.16:1807.16] It can't be your structure is really continuing to the notification of power that you have.
[1807.16:1813.16] If you think about it, really means our brains are super-duper efficient.
[1813.16:1814.16] I don't know.
[1814.16:1819.16] I always thought the fact is in a way that we could learn anything from neuroscience,
[1819.16:1826.16] but it's got to be simple efficiency.
[1826.16:1830.16] I think about the models that you can see.
[1830.16:1836.16] It's, I don't know, four million dollars to find.
[1836.16:1839.16] It doesn't fit in the job.
[1839.16:1848.16] But imagine standing four million dollar, HP, any human being.
[1848.16:1854.16] You think that person would create that much being that.
[1854.16:1857.16] We have that much being that emissions.
[1857.16:1861.16] I don't know, like for four million dollars.
[1861.16:1869.16] I think you did a good job on this. I'm the company kind of depends.
[1869.16:1874.16] And I will say that human would generalize out of the process.
[1874.16:1880.16] I don't know, from thriving to answering questions in a few jokes.
[1880.16:1883.16] You know, you will see.
[1883.16:1885.16] Again, I said it just to get jammed and lentil.
[1885.16:1888.16] It's a bit more like a clear circuit.
[1888.16:1891.16] Oh,
[1891.16:1895.16] All right.
[1895.16:1900.16] Then I hope that my senses will go wrong.
[1900.16:1904.16] I think it is.
[1904.16:1906.16] All right.
[1906.16:1910.16] So then.
[1910.16:1915.16] Why the solution is.
[1915.16:1917.16] What so.
[1917.16:1921.16] So to think about it, the solution is not only by the computation,
[1921.16:1925.16] because we had this to use a given point of change, but let's part this.
[1925.16:1930.16] So it's a very tiny.
[1930.16:1933.16] It's the visual purpose.
[1933.16:1938.16] So if you think about it, you know, the kind of circuitry in your brain that are.
[1938.16:1940.16] I highly recommend you basically.
[1940.16:1943.16] This shows you have an obstacle to reach it.
[1943.16:1949.16] So the visual stimuli comes from this goes all the way to the back of your head.
[1949.16:1953.16] And in the meantime.
[1953.16:1956.16] And then.
[1956.16:1959.16] Some solutions.
[1959.16:1964.16] And there's like all sorts of research and at the same time.
[1964.16:1967.16] There's a short talk to your.
[1967.16:1970.16] I think I talked about.
[1970.16:1973.16] What.
[1973.16:1979.16] You're going to have your fight or flight response before your obstacle.
[1979.16:1984.16] The original interpretations object is a 3d object.
[1984.16:1986.16] So whatever.
[1986.16:1987.16] Color.
[1987.16:1989.16] But not.
[1989.16:1994.16] You're going to go over the next decision like tests.
[1994.16:2001.16] Again, some sort of match filter in which is somewhat shaped by your past experience.
[2001.16:2007.16] That's why when I tell you, like, when you look at all questions in the example,
[2007.16:2014.16] over the past experience, the forms and sort of a stare or something like this.
[2014.16:2018.16] Think about the definition and think about what you have.
[2018.16:2020.16] You also have a cheap shooting.
[2020.16:2022.16] You're able to do certain things about it.
[2022.16:2023.16] Don't worry about it.
[2023.16:2026.16] But I digest that again.
[2026.16:2028.16] The point about this convolution.
[2028.16:2032.16] Somehow it seems that there's something called an inductive bias.
[2032.16:2036.16] You can learn the bias.
[2036.16:2040.16] If you divide it, you have some function plots that you create.
[2040.16:2045.16] So maybe to the connected certain activation from the team and so forth.
[2045.16:2051.16] But if you know that they exist already, the system that does well.
[2051.16:2055.16] So maybe the commemorations you have another function.
[2055.16:2063.16] If you're making this function plots, restricting what you can have.
[2063.16:2066.16] Maybe the true function.
[2066.16:2069.16] This closer.
[2069.16:2070.16] You know.
[2070.16:2075.16] So think about the worst systems here, which is here.
[2075.16:2081.16] So the error you can make within this at worst is this distance, right?
[2081.16:2093.16] And maybe the best there in this box is comparable.
[2093.16:2095.16] Think about it.
[2095.16:2099.16] So it's important what the architecture is.
[2099.16:2109.16] I'm dropping names like architecture here.
[2109.16:2117.16] And you're fixing the same number of farmers.
[2117.16:2131.16] But this makes sense.
[2131.16:2141.16] And then actually.
[2141.16:2146.16] Those are results.
[2146.16:2151.16] To be more expressive in the press for solving figures.
[2151.16:2153.16] Harder problems.
[2153.16:2157.16] The end up getting a lot of problems.
[2157.16:2162.16] And then you can see the same thing.
[2162.16:2165.16] I don't know if you see any action.
[2165.16:2168.16] You can get a paragraph about.
[2168.16:2175.16] And it'll fill in little predicts things like, oh, you know, there was a car team.
[2175.16:2177.16] And then there was like a vaccination.
[2177.16:2181.16] I just make stuff up.
[2181.16:2191.16] And then you know, you know, you know, you remember, well, I don't think I talk about the security.
[2191.16:2194.16] By the way, the test.
[2194.16:2201.16] So the deeper things around it.
[2201.16:2203.16] People just talk about this.
[2203.16:2220.16] Yeah, so.
[2226.16:2231.16] Yeah.
[2231.16:2238.16] This is a review about fine methods of it.
[2238.16:2243.16] Alright, again, hi there, this.
[2243.16:2250.16] And here's the steering of some of the models.
[2250.16:2254.16] I really like that people are good for their work.
[2254.16:2264.16] You, which you look at this particular steering, look at the parameters, you know, so in terms of millions.
[2264.16:2274.16] So, 1000 times millions is a billion, right.
[2274.16:2277.16] And then you grow all the way to trillion.
[2277.16:2287.16] And then I'm not advocating model and to simply exist because problem between training these models just.
[2287.16:2304.16] Alright, now.
[2304.16:2311.16] Think about a good deep learning problem, let's say, H of H sub X is a neural network.
[2311.16:2323.16] We have some samples labels and pixel visualization, the particular organization where it is there, a variety of both sections.
[2323.16:2329.16] So what is different from before, as opposed to having a linear model, we're generalizing the model.
[2329.16:2342.16] And now, how do you move that for the neural network is learning a little bit faster, we have layers, we're going to fail to speak that in.
[2342.16:2349.16] And then maybe the connection with this one has been that we want.
[2349.16:2363.16] And then, I think that one makes problems that you know how to go to the minimum, you know.
[2363.16:2373.16] So, I think this is from. I should help this system.
[2373.16:2400.16] And from both of them. These are you and you can visualize some of these optimization methods and they don't look from that.
[2400.16:2413.16] And anybody who is doing optimization, we do have this kind of optimization and state.
[2413.16:2428.16] So the conventional system machine learning was that he preferred simple models, the simpler functions, as opposed to neural networks.
[2428.16:2435.16] So the deep learning paradigm is we have massive data sets.
[2435.16:2440.16] We will somehow introduce this inductive bias.
[2440.16:2444.16] And use for casted methods, these are the two ingredients.
[2444.16:2453.16] I will tell you some extenuating circumstances in the sunset, which you set up your neural network correctly, this landscape becomes.
[2453.16:2458.16] And then we will talk about that.
[2458.16:2462.16] Something called overfront information.
[2462.16:2467.16] I think this is a good time to take this break.
[2467.16:2471.16] What I will do is I'm going to tell you the challenges.
[2471.16:2486.16] Now we'll do a bit more with the stuff and maybe a little bit less to a little bit more with us.
[2486.16:2489.16] There's a question.
[2489.16:2493.16] Oh, I thought up to record. No, okay. I didn't.
[2493.16:2502.16] Right now we're getting restarted. I started the recording again. So apparently I did not forget to report which I'm very happy about.
[2502.16:2509.16] Or maybe I did an ego basically picked up the slack.
[2509.16:2514.16] All right.
[2514.16:2517.16] So.
[2517.16:2529.16] I will give you a little bit of a summary of some of the challenges in the current people in paradigm.
[2529.16:2539.16] And some of these challenges are mathematically uncertain.
[2539.16:2543.16] So we'll start with the robustness challenge.
[2543.16:2548.16] I think it's a little bit more difficult to see.
[2548.16:2551.16] This particular example. So here.
[2551.16:2554.16] I think there's a turtle and not a tortoise.
[2554.16:2557.16] I did the difference between them once and the other one does not.
[2557.16:2559.16] I think it's important.
[2559.16:2561.16] I think this is a turtle.
[2561.16:2562.16] I think.
[2562.16:2564.16] But the point about this is that.
[2564.16:2567.16] Here you can train a neural network.
[2567.16:2573.16] And by putting performance.
[2573.16:2576.16] And by adding a little bit of an impressive.
[2576.16:2578.16] You can make a neural network.
[2578.16:2584.16] I think that.
[2584.16:2586.16] It was also this.
[2586.16:2600.16] I think that.
[2600.16:2609.16] What that means is that you have a lot of this way.
[2609.16:2617.16] I think that.
[2617.16:2619.16] I have a little bit of noise.
[2619.16:2621.16] You as a human.
[2621.16:2622.16] You like that.
[2622.16:2625.16] You have turtle or focus.
[2625.16:2628.16] But the neural network goes right.
[2628.16:2629.16] Yeah.
[2629.16:2630.16] This kind of.
[2630.16:2632.16] It's kind of tough.
[2632.16:2633.16] No.
[2633.16:2635.16] In terms of robustness.
[2635.16:2641.16] And the other one is I think the test engineers demonstrated is that.
[2641.16:2644.16] We put a bunch of stickers on a stop sign.
[2644.16:2646.16] Also in.
[2646.16:2650.16] In Spanish, I think.
[2650.16:2653.16] And all of a sudden your driver,
[2653.16:2663.16] I think it's dry automated driver in your test drive, the new plate that does one to come to the one thing 99 seconds.
[2663.16:2666.16] And then you can see that.
[2666.16:2669.16] Well, just.
[2669.16:2670.16] Roll through.
[2670.16:2671.16] Three down fast.
[2671.16:2673.16] Imagine the.
[2673.16:2675.16] The problem.
[2675.16:2677.16] So while.
[2677.16:2681.16] Neural networks of thing again beyond tougher performance and vision.
[2681.16:2682.16] Passing.
[2682.16:2683.16] Not necessarily.
[2683.16:2685.16] Robust.
[2685.16:2689.16] And I'm going to soil the phone here and say that this is not.
[2689.16:2693.16] And I think it's not necessarily like tennis.
[2693.16:2695.16] So here's a problem.
[2695.16:2698.16] Simple binary classification problems.
[2698.16:2703.16] Is stylized example to make the case, but I think that you can see where this is going.
[2703.16:2706.16] So he's a cats and dog example.
[2706.16:2707.16] All right.
[2707.16:2708.16] So I take a disk.
[2708.16:2709.16] I didn't do.
[2709.16:2710.16] I put.
[2710.16:2716.16] In a form data that I call as cats and put on the other hemisphere.
[2716.16:2720.16] And then you know, for data, I call dogs.
[2720.16:2722.16] Just entertaining.
[2722.16:2723.16] I just take this.
[2723.16:2725.16] That's some dogs example.
[2725.16:2730.16] We don't live in this particular disk, but for this particular purpose.
[2730.16:2736.16] What does it mean?
[2736.16:2741.16] To add in perceptible noise and change the decision.
[2741.16:2747.16] Which is the author.
[2747.16:2750.16] So imagine.
[2750.16:2754.16] For this particular line is our decision.
[2754.16:2759.16] By imperceptible noise, I'm just going to mean that something small like an excellent error.
[2759.16:2761.16] That's a visit to dimension.
[2761.16:2764.16] What does it mean to swap decisions?
[2764.16:2770.16] So this means that you can take an excellent fly sewer.
[2770.16:2776.16] Anything in this thing is in danger of.
[2776.16:2778.16] The multiple.
[2778.16:2779.16] No, no.
[2779.16:2780.16] No.
[2780.16:2783.16] Changing the decision.
[2783.16:2784.16] Yeah.
[2784.16:2787.16] Does that make sense?
[2787.16:2792.16] I'm literally constructing a synthetic stylized simple example.
[2792.16:2794.16] About this.
[2794.16:2797.16] What they put adversarial perturbation.
[2797.16:2803.16] With a small perturbation, you can take a door on one side.
[2803.16:2805.16] And make it a cat.
[2805.16:2811.16] But which one of the dogs can you do that with an imperceptible.
[2811.16:2815.16] Well, only within that excellent slide.
[2815.16:2820.16] You can take this and add an excellent to make it a cat.
[2820.16:2824.16] Is what I mean.
[2824.16:2827.16] Yes.
[2827.16:2831.16] Yes.
[2831.16:2839.16] Now, if you think about it, you know, these perturbations, you take them and put a in our case in this process, a.
[2839.16:2842.16] 8 in the deep learning the trick.
[2842.16:2846.16] I add just a little perturbation.
[2846.16:2848.16] What does it happen?
[2848.16:2851.16] You would have the origin function times epsilon.
[2851.16:2854.16] The gradient, you know.
[2854.16:2857.16] But you think about it.
[2857.16:2861.16] The live justness of this function.
[2861.16:2864.16] It turns on how the gradients change.
[2864.16:2866.16] For example, the function was smooth.
[2866.16:2869.16] Maybe it's more perturbations don't change much.
[2869.16:2875.16] In fact, this is this particular research topic for the live just cross information.
[2875.16:2880.16] The variety of ways of estimating these systems.
[2880.16:2885.16] And it's prior certifying the author of the neural network.
[2885.16:2887.16] This is yet to research.
[2887.16:2888.16] Right.
[2888.16:2894.16] But what I'm going to talk about now is the difficulty of the sale of systems of the sale of perturbations.
[2894.16:2897.16] The problem sense that here.
[2897.16:2904.16] What volume of the data can you expect with the next one size.
[2904.16:2911.16] How do you say very well?
[2911.16:2914.16] I'm also some of the conditions.
[2914.16:2918.16] The area under the external slice.
[2918.16:2922.16] The rest of the area would give us an idea about the percentage of the data points.
[2922.16:2928.16] If you were up to the net to say a perturbation.
[2928.16:2929.16] Yes.
[2929.16:2930.16] Yes.
[2930.16:2934.16] Please.
[2934.16:2938.16] And it's been small here.
[2938.16:2942.16] No, small finding.
[2942.16:2945.16] Except.
[2945.16:2949.16] If you take high dimensional sphere and take an external slice.
[2949.16:2958.16] It turns out that most of the volume of the sphere actually does indeed leave in that slice.
[2958.16:2963.16] This is called the blessing of the nationality.
[2963.16:2964.16] Yeah.
[2964.16:2968.16] Concentration of measure is the stuff that.
[2968.16:2972.16] This is love.
[2972.16:2975.16] By the way.
[2975.16:2981.16] Supplementary material on concentration of measure in qualities.
[2981.16:2989.16] It's a matter of.
[2989.16:2991.16] Advanced material.
[2991.16:2993.16] Not required.
[2993.16:2997.16] Not going to be asked if you want to learn more about concentration of measure.
[2997.16:3000.16] It was a whole supplementary literature.
[3000.16:3002.16] From advanced topics.
[3002.16:3004.16] Of course, I had from five years ago.
[3004.16:3007.16] So the point.
[3007.16:3015.16] That if you start increasing the dimensions to 200 for example, the point one slice.
[3015.16:3017.16] We'll have 85% of the volume.
[3017.16:3021.16] You make that dimension and image dimension was the idea.
[3021.16:3022.16] No, mega.
[3022.16:3026.16] Sorry.
[3026.16:3027.16] And mega.
[3027.16:3032.16] 99.99% of the volume will live in that excellent slice.
[3032.16:3037.16] And you can perturb the slides for to be patient.
[3037.16:3041.16] I'm sorry.
[3041.16:3045.16] And you can literally make the text.
[3045.16:3050.16] Among I say that all you know what kind of cheating because there is not.
[3050.16:3056.16] I mean, the decision boundary could be different for us.
[3056.16:3058.16] And I will tell you,
[3058.16:3064.16] I will tell you that this is actually the best decision boundary that you can vote for.
[3064.16:3068.16] Any other decision boundary will make the issue worse.
[3068.16:3071.16] I could say that a union decision boundary.
[3071.16:3076.16] It will be the poor divorce for it.
[3076.16:3079.16] I just realized it's not interesting.
[3079.16:3084.16] So what do you I mean this never prevents the showering the searches to robust.
[3084.16:3093.16] All in a bit of a sense some non uniform distribution that might help.
[3093.16:3097.16] Is some from the mental thing that you cannot.
[3097.16:3100.16] So you really get.
[3100.16:3103.16] Just the vacation for complicated problem.
[3103.16:3107.16] Know that there will be some more.
[3107.16:3110.16] This could be.
[3110.16:3114.16] So I'm not going to say that.
[3114.16:3116.16] All right.
[3116.16:3120.16] As a result, this is in fact a very.
[3120.16:3123.16] I don't know why you start from one to 10 again.
[3123.16:3126.16] So why not continue the Monday.
[3126.16:3129.16] There are 25 citations.
[3129.16:3135.16] It's what I mean there is an active research area.
[3135.16:3141.16] So there's there's punch issues with this.
[3141.16:3143.16] That's.
[3143.16:3146.16] The privacy concerns take some data sets.
[3146.16:3150.16] If you have also included the code of fair.
[3150.16:3152.16] I'm just dropping names.
[3152.16:3158.16] That by, for example, giving around the main course and keeping in and out the same.
[3158.16:3161.16] You can discover data.
[3161.16:3166.16] In that training data.
[3166.16:3169.16] New network.
[3169.16:3172.16] You can start it for some.
[3172.16:3180.16] Your face negative out of the human.
[3180.16:3184.16] As a result, this is actually quite important.
[3184.16:3187.16] The search area.
[3187.16:3192.16] Not alone.
[3192.16:3194.16] No.
[3194.16:3201.16] New methods are as fair as the data can be lost country.
[3201.16:3203.16] Now, give you one example.
[3203.16:3205.16] So there is a system.
[3205.16:3207.16] I can I put it back to.
[3207.16:3210.16] You know, but they realize the season of judges.
[3210.16:3213.16] The time.
[3213.16:3218.16] In order to help these judges, the system.
[3218.16:3221.16] The last season.
[3221.16:3223.16] Age.
[3223.16:3226.16] The decisions.
[3226.16:3231.16] You look at this.
[3231.16:3235.16] All right.
[3235.16:3240.16] You know.
[3240.16:3248.16] You would be updated and changing what.
[3248.16:3253.16] You know, these will eventually be changing the situation.
[3253.16:3257.16] On this day, the defendant.
[3257.16:3261.16] For this education or school as your private nữa.
[3261.16:3259.16] So, maybe you will go this morning Baruch already opened the room right away.
[3259.16:3266.16] The examination.
[3266.16:3279.02] Same thing you can see it's here.
[3279.02:3284.8799999999997] One domestic violence aggravated assault, grand theft, petty theft, drug trafficking,
[3284.8799999999997:3288.3599999999997] grand theft, the subsequent offense lowers.
[3288.3599999999997:3295.8399999999997] One petty theft, non-missbumes.
[3295.84:3320.42] I have complex suicide and violence obsession with views of murder and death which
[3320.42:3322.54] is up in a mere basement.
[3322.54:3324.54] All right.
[3324.54:3326.54] I'll tell you just.
[3326.54:3335.54] Okay.
[3335.54:3342.54] I'm sorry.
[3342.54:3346.54] Like you order through the details system. It takes months.
[3346.54:3356.54] I know.
[3356.54:3363.54] So, the point I want to make is the following. So in the end.
[3363.54:3366.54] This is simple classification problem.
[3366.54:3369.54] Equal number of circles versus.
[3369.54:3374.54] And we're on all the way like this. We get the middle.
[3374.54:3379.54] As your decision, it will be the maximum margin classifier.
[3379.54:3388.54] That the margin between the two data sets could be next.
[3388.54:3390.54] I did the point here.
[3390.54:3395.54] It should be classified as.
[3395.54:3401.54] If you have more data of unless data inside.
[3401.54:3403.54] It's an actually make.
[3403.54:3407.54] The classification boundary not the geometric max margin.
[3407.54:3411.54] Because of the other ones, we use like the specific gradient descent.
[3411.54:3416.54] The decision boundary can get closer to the inner.
[3416.54:3420.54] So the same data point here should they be classified as this.
[3420.54:3431.54] If you push out a little bit, the decision boundary will even further push down the decision boundary closer to the minority.
[3431.54:3434.54] And hence it's perpetuating.
[3434.54:3443.54] Facing unfair.
[3443.54:3453.54] Life is tough already.
[3453.54:3459.54] What am I students is when you are told this particular problem.
[3459.54:3462.54] The forward picture.
[3462.54:3468.54] This is a spanner for example, there is a major.
[3468.54:3473.54] Interprety.
[3473.54:3476.54] Imagine.
[3476.54:3480.54] You write papers like this.
[3480.54:3482.54] Not.
[3482.54:3484.54] Match stuff here.
[3484.54:3485.54] And in between.
[3485.54:3488.54] And you know,
[3488.54:3498.54] the first one is.
[3498.54:3499.54] I'm fortunate.
[3499.54:3501.54] I'm fortunate.
[3501.54:3503.54] And they're good.
[3503.54:3506.54] We stopped the gradient and turned the computer.
[3506.54:3507.54] So while training this.
[3507.54:3508.54] And we got this result.
[3508.54:3518.54] And we're going to do this.
[3518.54:3519.54] We think about it.
[3519.54:3522.54] We go from simple models to more complicated models, the accuracy of thesis.
[3522.54:3523.54] But.
[3523.54:3526.54] In the sense that you can't really explain what is going on.
[3526.54:3530.54] Are there either coaches like integrated graders and some of their.
[3530.54:3531.54] Regretions.
[3531.54:3534.54] At some idea, but the particular problem.
[3534.54:3544.54] And then you give decision.
[3544.54:3546.54] And then you give decision.
[3546.54:3547.54] Or.
[3547.54:3549.54] Introduce the decision.
[3549.54:3551.54] You want to be able to explain it.
[3551.54:3553.54] You can be discriminated.
[3553.54:3554.54] You know,
[3554.54:3556.54] decision based on valid reasons.
[3556.54:3557.54] And you're not so far.
[3557.54:3558.54] I was told.
[3558.54:3559.54] Challenge.
[3559.54:3564.54] And then it was here the fact on a new network.
[3564.54:3565.54] Right.
[3565.54:3566.54] The state of jizou may.
[3566.54:3567.54] And maybe do a system.
[3567.54:3568.54] One of the words.
[3568.54:3569.54] And then.
[3569.54:3570.54] Or is.
[3570.54:3572.54] It picked up automatically.
[3572.54:3579.54] Because we're alive.
[3579.54:3580.54] Active.
[3580.54:3582.54] Active research area.
[3582.54:3586.54] An energy and false.
[3586.54:3589.54] And then.
[3589.54:3591.54] So here's the funny business.
[3591.54:3593.54] So this is a very, very nice.
[3593.54:3594.54] This is the.
[3594.54:3597.54] The number of GPUs needed.
[3597.54:3600.54] The process incoming YouTube data.
[3600.54:3602.54] Here's the growth of data.
[3602.54:3604.54] And here's the growth.
[3604.54:3610.54] Of the needed computation seems to be a widening gap.
[3610.54:3614.54] So if the amount of data outflows the amount of computation.
[3614.54:3617.54] That's what we have.
[3617.54:3618.54] That's the problem.
[3618.54:3623.54] If you can manage in some of the problems may take increasingly
[3623.54:3624.54] longer time to solve.
[3624.54:3626.54] Or.
[3626.54:3628.54] We would burn the planet to solve them.
[3628.54:3631.54] That way the planet will be perfect for the machines to live in.
[3631.54:3633.54] No.
[3633.54:3640.54] The park.
[3640.54:3642.54] Again.
[3642.54:3645.2599999999998] the active research area.
[3645.2599999999998:3649.2599999999998] And now the time is 941, so we can do a little bit of math.
[3649.2599999999998:3653.2599999999998] I'm going to start feeling the only thing
[3653.2599999999998:3656.86] that makes your eyes watery.
[3656.86:3658.86] But in the end, you're going to develop
[3658.86:3661.38] the taste of the enemies even.
[3661.38:3664.3] So let's speak about the actions.
[3664.3:3667.46] Supervised gun and iodine.
[3667.46:3671.98] Here, we have some generator, the genius
[3671.98:3673.26] and data.
[3673.26:3676.18] Supervised the God during the some labels.
[3676.18:3679.22] And as the learning machine, our job
[3679.22:3681.82] is to learn some function and acting.
[3681.82:3685.94] Now, the tab we have here or the definitions.
[3685.94:3690.86] OK, so we're going to define a metric over the function
[3690.86:3691.62] spaces.
[3691.62:3694.18] We're going to have some two function,
[3694.18:3700.38] we do some two function class that minimizes our risk.
[3700.38:3703.1800000000003] Now, let's say that we don't know what the two function
[3703.1800000000003:3706.54] class is, if you have some function class like unit
[3706.54:3710.78] works, let's say this H natural is our solution
[3710.78:3715.26] under the function class, the best unit.
[3715.26:3719.78] That's the best known commuter unit metric.
[3719.78:3723.1800000000003] Now, as engineers, scientists,
[3723.1800000000003:3725.1] and sort of organization populations,
[3725.1:3727.1800000000003] and then optimize them, let's say
[3727.18:3730.58] the APE star is the optimization solution.
[3730.58:3734.1] And then you know that that's the hypothetical solution
[3734.1:3738.46] if you had the perfect algorithm that can go all times
[3738.46:3742.3799999999997] of both the minimum and give us an optimal solution.
[3742.3799999999997:3743.22] But we don't.
[3743.22:3745.8199999999997] If we have an algorithm that will do some iterations,
[3745.8199999999997:3749.18] we do graduate students to sound on the algorithm,
[3749.18:3753.8199999999997] architecture, the data, the feature engineering.
[3753.82:3757.5800000000004] And in the end, this is what we have, H of P,
[3757.5800000000004:3762.38] run something with an education.
[3762.38:3766.1400000000003] Some energy shows those little lines
[3766.1400000000003:3769.1000000000004] that you will be looking at in your computer,
[3771.5800000000004:3773.5] and so on and so forth.
[3775.5:3777.42] But what do we care about?
[3777.42:3781.6200000000003] We care about the distance of what we have in our hand
[3781.62:3783.9] to the two function.
[3783.9:3787.46] Now we apply the second position of error.
[3787.46:3790.8199999999997] You can write that into three distinct terms.
[3790.8199999999997:3792.8199999999997] One is the optimization error.
[3796.42:3798.5] Idealize estimator.
[3798.5:3801.42] The algorithm is trying to approximate the estimator.
[3802.42:3806.74] This estimator is trying to approximate the two function
[3807.7:3810.58] with any different function class.
[3810.58:3814.74] And the function class is trying to estimate the two
[3814.74:3816.42] in real step red.
[3826.74:3831.74] So then we truly identify two parts of error here.
[3831.74:3833.66] One is the optimization error.
[3833.66:3837.9] The other one is the statistical error and the model error.
[3837.9:3840.78] And of course, the type of sphere is the show.
[3840.78:3845.34] So ideally, we can reduce the optimization error
[3845.34:3846.7000000000003] with expectation.
[3846.7000000000003:3850.46] I'm still ideally, because non-con�ate problems,
[3850.46:3851.46] I don't know.
[3851.46:3854.7000000000003] We can put more computation, but we may not necessarily
[3854.7000000000003:3856.98] in two on the error.
[3856.98:3858.14] Again, ideally.
[3860.14:3862.02] We can reduce the statistical error
[3862.02:3864.7400000000002] with more data samples, better estimation,
[3864.74:3868.58] and the prior information, sure, how buy that.
[3868.58:3869.66] Yeah?
[3869.66:3872.02] With more data, things are more better.
[3875.8199999999997:3877.7] And then, between reduce the model error,
[3877.7:3879.9799999999996] red flexibly universal representations
[3879.9799999999996:3883.58] and hence the church of the, I don't know,
[3883.58:3888.58] the religion off the new Netflix.
[3891.7:3892.7] Okay.
[3892.7:3895.7] Now, again, I did go for notation.
[3903.54:3906.54] I think this is the same.
[3906.54:3908.54] I think this is the same.
[3908.54:3909.38] Okay.
[3909.38:3911.1] So what are the elements of our problem?
[3911.1:3913.58] Now, we have a primary model.
[3913.58:3916.42] The list that they're interested in is this X-axis population
[3916.42:3919.3399999999997] with this over a lost function that we picked.
[3919.3399999999997:3922.94] So in the case of the primary model,
[3922.94:3926.94] we have, you know, what is a new Netflix new canvas?
[3926.94:3929.2599999999998] Anybody see black in here?
[3929.2599999999998:3931.2599999999998] Did the property changes name?
[3931.2599999999998:3933.2599999999998] So you go from H to X-axis?
[3934.74:3936.54] So, say, this color is from them.
[3936.54:3937.54] Okay.
[3937.54:3938.54] Okay.
[3938.54:3941.7] So as opposed to having H-not,
[3941.7:3945.3] we're going to talk about X-not by next model.
[3949.3:3950.58] I am stupid story.
[3953.1:3954.66] Maybe I'll borrow somebody else's eye,
[3954.66:3966.66] let's go.
[3969.14:3973.14] So it's natural, it's star, it makes B.
[3977.3399999999997:3984.02] Now, the thing that is interesting is that, you know,
[3984.02:3987.38] we have a two-risk expectation,
[3987.38:3990.06] the lost function, the particular armature,
[3990.06:3993.3] which we have now, and instead we
[3993.3:3996.58] encrypt this estimator, which is the primary model,
[3996.58:3998.2599999999998] the primary sample.
[3998.2599999999998:4001.82] So I'm going to define the encrypt this as r sub n,
[4001.82:4004.06] and explicitly write the base dependence.
[4006.86:4009.74] So r sub n is our training error.
[4009.74:4011.18] If you think about it, the population
[4011.18:4014.18] is r sub n is r sub n.
[4017.18:4019.98] Now, what is our modeling error?
[4019.98:4026.5] Our modeling error is that the two model versus what
[4026.5:4030.5] is shown within the function pass.
[4035.8999999999996:4039.18] And then the X-axis risks is when we do the n-picture
[4039.18:4041.3399999999997] risk minimization, we obtain a parameter,
[4041.3399999999997:4045.94] and we just take the true risk and we
[4045.94:4049.8199999999997] assumed model the stiffness.
[4054.22:4056.7] What is known as the generalization error
[4056.7:4061.3399999999997] is basically the supremum over your function pass
[4061.3399999999997:4066.62] the stiffness between the risk, the population
[4066.62:4071.8599999999997] is 0, this is meant to be risk.
[4071.8599999999997:4074.22] Over endings.
[4074.22:4076.42] And then our optimization error is, of course,
[4076.42:4080.9] this is the minimum risk, and this is the optimization error
[4080.9:4081.42] risk.
[4084.7799999999997:4086.38] Here's a table that's training, and we
[4086.38:4087.8599999999997] fix one of the variables.
[4087.8599999999997:4091.5] How does the other is, sorry,
[4091.5:4093.2999999999997] and it fix all the other variables.
[4093.2999999999997:4096.5] How does the risks errors,
[4096.5:4097.5] success, behave?
[4103.02:4103.54] All right.
[4109.42:4111.46] So if you think about the practical defenders
[4111.46:4113.22] of what we could be in deep learning,
[4113.22:4120.22] is that you care about our generalization.
[4120.22:4124.82] So there exists a two X-not parameter.
[4124.82:4127.259999999999] We have an algorithm.
[4127.259999999999:4128.299999999999] Yeah.
[4128.299999999999:4131.259999999999] We have these parameters, and we care about how well
[4131.259999999999:4135.54] we will do, we just take this in terms of the population.
[4135.54:4136.54] Yeah.
[4136.54:4139.219999999999] So this is not error, our element.
[4139.219999999999:4143.86] This is the C-risk R-expected value.
[4143.86:4147.259999999999] And then you can do this error decomposition,
[4147.259999999999:4148.78] and at the end of the literature,
[4148.78:4151.0199999999995] it shows how to do this error decomposition.
[4151.02:4155.02] If you get just two times a secluded, it's not
[4155.02:4156.42] impopticated.
[4156.42:4159.26] I won't certainly ask me when it's now.
[4159.26:4161.38] That is a nice particular data for the evaluation.
[4167.14:4170.06] Look at this optimization error, generalization error,
[4170.06:4174.660000000001] and model error, similar kind of composition.
[4174.66:4175.66] OK.
[4180.5:4181.54] It is interesting.
[4181.54:4186.22] So in deep learning, you can make your optimization error
[4186.22:4187.22] almost zero.
[4193.7:4195.34] And there is a means for it.
[4195.34:4197.42] Because if you look at your optimization error,
[4197.42:4204.46] it's not within terms of R, but R sub N.
[4204.46:4207.86] You can book about this in lecture nine.
[4207.86:4210.42] The intensity was like overcoming position.
[4215.34:4219.82] Generalization error, these bam things,
[4219.82:4229.02] generalize well, somehow, but the theory is very primitive.
[4229.02:4236.780000000001] You will see the primitive field to talk about a little bit
[4236.780000000001:4241.5] in this lecture as time permits, and the literature
[4241.5:4243.660000000001] in simple algorithmic stability.
[4247.9800000000005:4251.9800000000005] And the last term, model error, large architectures,
[4251.9800000000005:4254.660000000001] as you know, the neural networks are universal foxes,
[4254.660000000001:4257.34] and hence making the hidden layer
[4257.34:4261.3] to grow exponentially, you can approximate any function.
[4261.3:4261.82] Any?
[4264.900000000001:4266.9400000000005] Or we thought about this optimization error
[4266.9400000000005:4271.78] that works in dot-trick files.
[4274.78:4277.42] Somehow, we're sticking the function class,
[4277.42:4281.9800000000005] doing graduate students, and the architectures, the cost.
[4281.9800000000005:4285.62] There's also a problem called neural architecture search.
[4285.62:4287.54] Some of you might know this a little bit.
[4287.54:4288.04] Yeah.
[4295.62:4296.62] All right.
[4299.78:4305.78] So how do you obtain a generalization?
[4305.78:4307.66] In this particular case, generalization
[4307.66:4316.0199999999995] bound means that how can you within your parameter class,
[4316.0199999999995:4321.74] how, you know, take the term of the proofing
[4321.74:4326.22] this is from through us.
[4326.22:4328.139999999999] And for this purpose, we're going to take
[4328.139999999999:4330.22] a little bit of this called as concentration in calculus,
[4330.22:4333.5] and there's a piece of proof of classification in calculus.
[4333.5:4336.74] So how do you move as a supplemental lecture?
[4336.74:4339.66] And so if you're interested in learning the theory,
[4339.66:4342.219999999999] or the more sophisticated concentration
[4342.219999999999:4344.66] of major inequalities, piece of the data
[4344.66:4348.179999999999] that's actually not in the formula, I'm not going to cover it.
[4348.179999999999:4352.62] I'm going to cover all new ones, the immediate label of it.
[4352.62:4354.86] And the flavor of this is like this.
[4354.86:4360.74] And the first case is going to hate it.
[4360.74:4362.66] And then you're going to see some of the settings
[4362.66:4366.66] that each open and running at this, comes with it.
[4366.66:4368.54] And start appreciating it more.
[4368.54:4371.94] You can have a color stroke.
[4371.94:4373.82] The one that we're going to focus on, the properties
[4373.82:4374.58] and quality.
[4378.86:4382.38] Maybe this ordained, the swap our arms.
[4382.38:4385.139999999999] So the properties and qualities to calling,
[4385.139999999999:4387.34] is suppose you have some idea.
[4387.34:4389.66] And then the animals, you look at the average,
[4389.66:4391.22] and click the average.
[4391.22:4395.62] Yep, so this is S. Can you think of this as a car M?
[4395.62:4398.780000000001] Now, arguably, this is not a fortune per say.
[4401.26:4402.62] I don't know what you're saying.
[4402.62:4404.14] This is the same formula.
[4413.860000000001:4417.5] I don't know that's going to be a name.
[4417.5:4423.94] I'm sorry.
[4423.94:4425.86] So I think about it this way.
[4425.86:4428.46] I'm going to fix X. And I'm going to pick about 12 R
[4428.46:4433.94] and change this from R. Is that in a sense?
[4437.06:4443.34] What's happening in quality is that the deviation from the name,
[4443.34:4447.18] that's used the expected value of this,
[4447.18:4448.5] which would be our risk.
[4448.5:4451.14] So imagine again, these things are swapped in order
[4451.14:4453.46] so that you have the proper name.
[4453.46:4455.740000000001] So this is that, and that is that.
[4455.740000000001:4456.740000000001] Yeah.
[4460.34:4463.54] So we're going to assume some boundary function.
[4463.54:4465.22] So the random name is a boundary.
[4465.22:4467.58] So think about, I don't know, maybe just the rest.
[4467.58:4468.820000000001] That's exactly.
[4468.82:4478.86] It says that this S and D be adding form,
[4478.86:4481.86] it's expected value, and by an amount of key,
[4481.86:4486.94] goes exponentially down in probability.
[4486.94:4489.94] That's the reason why we're boundedness.
[4492.98:4494.7] And to be honest with you, I think
[4494.7:4499.38] we should put a tall chart here.
[4499.38:4505.7] He has a nice and awesome, the fastest, the awesomeness
[4505.7:4511.58] that he is on this user-friendly concentration balance.
[4511.58:4515.98] I know Joe, but deeply about this and quickly
[4515.98:4520.139999999999] did it with something that he writes beautifully.
[4520.139999999999:4522.62] Again, I'll repeat this my research either.
[4522.62:4524.66] He writes beautifully.
[4524.66:4528.46] I highly recommend you look at those papers
[4528.46:4530.5] or he's got the kind of information
[4530.5:4534.42] that he's asking on this beautiful, beautiful, beautiful.
[4540.58:4544.94] So the deviation to clear is that if you have more and more,
[4544.94:4547.5] it goes down that way.
[4547.5:4553.1] For the both he writes, for any fixed key, I think this here,
[4553.1:4556.26] you can follow the data aspects to deviation.
[4561.46:4563.82] By the way, you can also have this kind of deviation
[4563.82:4567.42] balance for matrix variables.
[4567.42:4570.46] Joe has it all.
[4570.46:4574.94] Because the neat chip that needs inequality,
[4574.94:4578.139999999999] there's strong functionalism always like this,
[4578.139999999999:4581.299999999999] a bunch that are really useful, that I highly recommend.
[4591.299999999999:4597.74] So it means that if you actually fix the problem of this,
[4597.74:4603.259999999999] so if you fix this to be delta, what
[4603.26:4607.1] you can do, all right?
[4614.22:4617.38] Who the reverse?
[4617.38:4626.66] Try to figure out how this key can be selected for a given
[4626.66:4634.26] delta is a function of N, not N factorials,
[4634.26:4635.98] so as a function of the data size,
[4635.98:4642.26] you can characterize how this deviation is going to be.
[4642.26:4643.34] Probably.
[4643.34:4645.5] So with one minus delta probability,
[4645.5:4651.46] so let's see, we have binary, like old,
[4651.46:4654.099999999999] we have this here on loss.
[4654.1:4660.06] We have more or two over delta divided by 2N.
[4660.06:4685.02] For a given x, for a given x, for a given x.
[4685.02:4692.02] So far.
[4692.02:4697.02] OK.
[4697.02:4710.02] Now, suppose we have finite mean many possibilities.
[4710.02:4715.02] All right, meaning that we didn't have one x, but what we do
[4715.02:4722.02] is the cover the parameters, space, and bunch of x's.
[4722.02:4723.02] Yeah.
[4723.02:4728.02] You guys know about covering and packing.
[4728.02:4736.02] So I did an area which can maybe pack some spheres.
[4736.02:4738.820000000001] If you have the technical number, the covering number
[4738.82:4742.82] will be just twice as good.
[4742.82:4745.82] Not twice the use to cover the area, for example.
[4745.82:4749.82] What's the best packing for a given bag?
[4749.82:4750.82] I mean, I'm just going to try.
[4750.82:4752.82] So something like this.
[4752.82:4755.82] I don't know if you heard about this.
[4755.82:4757.82] What the kitchen for design of the shape of in the
[4757.82:4758.82] men chapter.
[4758.82:4759.82] So, I think.
[4759.82:4761.82] One of these.
[4761.82:4765.82] A few stories that I never know to this prove that kind of sounds
[4765.82:4770.82] like this.
[4770.82:4774.82] Anyway, so what you do is that, you know, in case if you know
[4774.82:4777.82] that this falls for one x.
[4777.82:4781.82] If you wanted to hold for all the x's in this particular set,
[4781.82:4785.82] if you use what is called as union bound, anybody remember the
[4785.82:4787.82] hand out then.
[4787.82:4794.82] The fact that this goes down exponentially.
[4794.82:4799.82] Again, get the load turned.
[4799.82:4805.82] The results.
[4805.82:4810.82] I got to tell you this like a bunch of deeper extensions.
[4810.82:4811.82] You can think about this.
[4811.82:4815.82] These things like doubly numbers and whatever.
[4815.82:4817.82] Them are two functional.
[4817.82:4822.82] This is like a deep puppet.
[4822.82:4825.82] The control means literally.
[4825.82:4829.82] Simple.
[4829.82:4834.82] Okay.
[4834.82:4838.82] This is I don't know any entry.
[4838.82:4843.82] I think that the generalization bond.
[4843.82:4848.82] You might be wondering what is this in the end is not this.
[4848.82:4854.82] The realization bond says this.
[4854.82:4857.82] More data.
[4857.82:4861.82] But it's what rate for all of us.
[4861.82:4863.82] As a generalization.
[4863.82:4867.82] We talked about how maximum what you create.
[4867.82:4870.82] Similar.
[4870.82:4874.82] Now can you imagine regularize and put this in the organization
[4874.82:4877.82] with this thing I'll run known what would be half here.
[4877.82:4880.82] So currently.
[4880.82:4884.82] P need the dimension or the complexity of the set is in the numerator.
[4884.82:4885.82] Yeah.
[4885.82:4887.82] If you just pass it in.
[4887.82:4890.82] You just take that complexity out.
[4890.82:4893.82] You can be completely in.
[4893.82:4899.82] That's how close like this is literally at a higher level.
[4899.82:4902.82] Your generalization will be spread of any.
[4902.82:4904.82] What does it need to be.
[4904.82:4907.82] The degrees of freedom in the problem.
[4907.82:4911.82] If your parameter set.
[4911.82:4915.82] Is a bunch of numbers with cardinality.
[4915.82:4918.82] Absolute value of the X.
[4918.82:4920.82] What's the degrees of freedom?
[4920.82:4923.82] How many bits does it take you to fault that set?
[4923.82:4926.82] Log and.
[4926.82:4932.82] So log X divided by.
[4932.82:4937.82] And so this is your generalization.
[4937.82:4940.82] And did you do anything sophisticated about fooling the.
[4940.82:4942.82] The reason it probably is something.
[4942.82:4947.82] But you know sometimes you have to stand on the shoulder of giants to make progress.
[4947.82:4948.82] So let's.
[4948.82:4950.82] Take the visit.
[4950.82:4953.82] Application of this is the bunch of lines.
[4953.82:4956.82] The patient of meeting.
[4956.82:4960.82] All right.
[4960.82:4963.82] So how do we assign.
[4963.82:4967.82] Remember something called the Dalson with.
[4967.82:4971.82] And I talked about assigning dimension like these.
[4971.82:4974.82] The number of things.
[4974.82:4975.82] Remember that.
[4975.82:4977.82] The Dalson with.
[4977.82:4983.82] The similar concept here is the rather market complexity.
[4983.82:4987.82] And the idea of the rather market complexity.
[4987.82:4990.82] The interest mentioned parameters.
[4990.82:4999.82] So what I've done was a part of the mentioned option.
[4999.82:5001.82] And now.
[5001.82:5004.82] And the continuous space.
[5004.82:5008.82] This is the rather market complexity.
[5008.82:5010.82] Is given.
[5010.82:5011.82] As.
[5011.82:5017.82] And the measures.
[5017.82:5019.82] For a given input step.
[5019.82:5021.82] So people.
[5021.82:5022.82] A.
[5022.82:5029.82] So I think there's a fact that this should be a.
[5029.82:5031.82] People I don't like to take notes.
[5031.82:5036.82] It's a fact.
[5036.82:5042.82] How well we can correlate with some random function.
[5042.82:5047.82] So this is a basic inner product between some random speed.
[5047.82:5049.82] So this is not an activation function.
[5049.82:5051.82] So maybe we should.
[5051.82:5054.82] We should change some notation here.
[5054.82:5055.82] But.
[5055.82:5057.82] There may be for the next situation.
[5057.82:5062.82] So here the signals are not the activation functions.
[5062.82:5064.82] I apologize for this.
[5064.82:5073.82] So what I'm looking is that can I take some random.
[5073.82:5079.82] Can I have a function plus that is expressed enough to fit any random.
[5079.82:5082.82] Yeah.
[5082.82:5087.82] If you have a rich function plus you can fit any random sequence.
[5087.82:5090.82] Whereas if you have a not so rich function plus.
[5090.82:5095.82] Maybe you won't get some.
[5095.82:5096.82] Yeah.
[5096.82:5101.82] Maybe you can get some other ones you cannot.
[5101.82:5111.82] So these are your latest labels for each of the data points.
[5111.82:5114.82] If you can fit any function, then.
[5114.82:5119.82] Then you take an inner product.
[5119.82:5124.82] We get the supremum over your function plus this is high.
[5124.82:5129.82] You need high leather marker and flexibility.
[5129.82:5139.82] Means that your function plus is rich and powerful.
[5139.82:5143.82] Kind of like the cooking practice.
[5143.82:5151.82] Which we will have in our lab.
[5151.82:5155.82] So.
[5155.82:5160.82] So high random often complexity means that you can pick any signs that you like.
[5160.82:5171.82] Over your function plus you can find a function that for each state of point is going to get that label.
[5171.82:5174.82] And then there is a lower the market.
[5174.82:5181.82] Means that your function class, even if you try to always.
[5181.82:5186.82] Because if you move over any function, you know, function class.
[5186.82:5189.82] It cannot fit these.
[5189.82:5191.82] It's this random numbers.
[5191.82:5195.82] And to be honest with you, this has the connections.
[5195.82:5201.82] The mean with the things that I talked about.
[5201.82:5206.82] We just pick the assignment and find like the mentioned part of the columns.
[5206.82:5209.82] To save things in this sense.
[5209.82:5211.82] Anyway.
[5211.82:5214.82] Yeah, just to slide.
[5214.82:5218.82] I'm going to take I apologize maybe another five minutes.
[5218.82:5219.82] It's okay.
[5219.82:5222.82] And then I will have that.
[5222.82:5226.82] The point that I want to make is that for.
[5226.82:5233.82] For the parameter, the national problem, the spaces again, we have a generalization bound.
[5233.82:5240.82] Using rather market complexity.
[5240.82:5251.82] And our generalization on the list depends on rather market complex with plus.
[5251.82:5254.82] On over.
[5254.82:5257.82] And.
[5257.82:5264.82] Meaning that your generalization somehow depends on the complexity of your function class.
[5264.82:5266.82] So.
[5266.82:5269.82] As your complexity increases, there's a term that we freeze this.
[5269.82:5271.82] So this is this.
[5271.82:5274.82] The market and.
[5274.82:5275.82] Yeah.
[5275.82:5278.82] As your data size increases, there's a term that.
[5278.82:5281.82] And.
[5281.82:5288.82] And people expect this kind of behavior.
[5288.82:5292.82] It's like a spot somewhere that you can find a function for us.
[5292.82:5296.82] Even the number of data points that you can pick.
[5296.82:5305.82] To give you the best generalization using these uniform bound.
[5305.82:5308.82] And then I must say again, on a first look.
[5308.82:5313.82] Or on a first case, this is what the phone.
[5313.82:5316.82] Now.
[5316.82:5318.82] So.
[5318.82:5322.82] Give it a bit.
[5322.82:5325.82] I'm not showing you.
[5325.82:5329.82] If the time can be an analogy, so this is not good.
[5329.82:5332.82] Then I would.
[5332.82:5336.82] So the.
[5336.82:5340.82] There's two competing things, you know.
[5340.82:5342.82] The function class.
[5342.82:5345.82] The function class.
[5345.82:5347.82] It's your generalization.
[5347.82:5351.82] The simpler function class that ended up small error.
[5351.82:5357.82] The large data is the best is what I'm trying to say.
[5357.82:5360.82] And.
[5360.82:5363.82] So the first one is a nice example here.
[5363.82:5367.82] I think the example is elementary enough that I believe to you.
[5367.82:5371.82] Just take a look at this particular slide.
[5371.82:5375.82] We will not be responsible for the exam.
[5375.82:5379.82] Now.
[5379.82:5384.82] Of course, we take this hammer and try to fly to your net worth.
[5384.82:5386.82] A couple of musical.
[5386.82:5390.82] And then we have a few different parts of the paper on this.
[5390.82:5395.82] That relates to particular complexity with the sticking on off the.
[5395.82:5397.82] Great.
[5397.82:5403.82] And in fact, there is a paper called fantastic generalization bounds and how to find them.
[5403.82:5405.82] And I'm not joking here.
[5405.82:5407.82] There is a paper.
[5407.82:5416.82] And then we have a lot of different generalization bounds and how to find them.
[5416.82:5424.82] So there are a bunch of definitions and complexity bounds.
[5424.82:5430.82] And the one that seems to be well-followed in the generalization is something for the path mode.
[5430.82:5433.82] And what path known to us is it looks at your input.
[5433.82:5437.82] And then we have a few paths that.
[5437.82:5440.82] That might be input to the output.
[5440.82:5446.82] So the smaller the one norm of these paths that go from the entity output.
[5446.82:5450.82] The daily generalization.
[5450.82:5452.82] And you that we're going to end.
[5452.82:5459.82] But I must say that there are some newer evaluations of complex measures.
[5459.82:5464.82] But it's still far from.
[5464.82:5468.82] Swaining new generalization.
[5468.82:5473.82] But I mean subtle things that are more layers of learning that we've got.
[5473.82:5474.82] Right.
[5474.82:5477.82] Have a great week.
[5477.82:5482.82] Recipation on Friday will be in PC one.
[5482.82:5484.82] I believe you go.
[5484.82:5486.82] You cannot stay.
[5486.82:5489.82] I'm looking to go.
[5489.82:5491.82] So have a great week guys.
[5491.82:5517.82] Thank you for choosing my book.
