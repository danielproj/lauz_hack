~EE-556 / Lecture 2 - 1/2 (2020)
~2020-09-28T15:32:00.689+02:00
~https://tube.switch.ch/videos/7bb3af3c
~EE-556 Mathematics of data: from theory to computation
[0.0:7.0] Today is a very, very important lecture of all times.
[7.0:11.0] What we're going to do now is we're going to start with the algorithms.
[11.0:20.0] And I would like to highlight that in the data processing pipeline,
[20.0:25.0] computation plays a large role.
[25.0:33.0] The thing that has made a big impact over the last decade or so was that people started thinking computationally, things that we can do,
[33.0:41.0] and then we've shown some very interesting results in neural networks, and then now the rest is history.
[41.0:44.0] Alright, so let's begin.
[44.0:51.0] What I'm going to do today is I'm going to talk a little bit about iterative optimization algorithms.
[51.0:58.0] We're going to talk about their efficiency for complex problems.
[58.0:61.0] I can define complexed in diversification.
[61.0:72.0] What I will do today is I'm going to maybe refresh your memories, and then we're going to talk about convergence for non-commax problems, whatever that may be.
[72.0:84.0] Just to recall the setting, if you recall, what we've been doing was to talk about the statistical learning theory framework,
[84.0:96.0] where we had a generator that generated these data points, A, we had a supervisor that was giving us some labels or real numbers or probabilities, as EI.
[96.0:108.0] And then our job as the learning machine was to understand this mapping of the supervisor, right, from the data to the labels, the probabilities to real numbers.
[108.0:124.0] For this course, we chose the probabilistic model in framework, where we assume that we somehow know a model for this EI given AI.
[124.0:129.0] And this model is parametric, but it has some unknown parameters.
[129.0:133.0] It's not a free model in the sense that we could not use any functions.
[133.0:141.0] We could only use those functions that are captured by some parameters set as defined by us.
[141.0:145.0] So here we write down the probability model.
[145.0:153.0] We assume that the data is given IID independent and identically distributed.
[153.0:161.0] Then we would write the joint probability of observing the data and relate that to our parameters.
[161.0:174.0] And one way of one principled way or a very good respect depends on whom you talk is the maximum likelihood estimator.
[174.0:194.0] We write down the negative look likelihood of this observing the data and the peak parameters that leads to the highest likelihood of observing the data that you observed.
[194.0:209.0] But in general, we don't have to use maximum likelihood. We can use whatever estimator we like and we talked about things like maximum likelihood type estimators or estimators or minimization type estimators.
[209.0:219.0] So here, you know, we minimize down function subject to some provider set within some constraint domain.
[219.0:234.0] Now let's focus on the computational course here. What we will do in this lecture is we're going to talk about minimizing some function f of x subject to no constraints.
[234.0:247.0] So this is a warm up. And as we progressed in our aptitude on optimization, we're going to add constraints, regularizers and all the good stuff.
[247.0:261.0] Just to give you some rotational clarity. So we will talk about minimizing. And as you see that I assign this capital F star as equal to minimizing this function.
[261.0:271.0] That means F star is the minimum value. And you will see oftentimes I use arg min. That means the argument of the minimizer.
[271.0:280.0] That may be unique in which case I tend to put equal. If it is not unique, I put this element of sign.
[280.0:286.0] Just to highlight the fact that there may be more than one minimizer.
[286.0:296.0] So here, S star is the solution set. And of course, this problem is a solution if the solution set is not.
[296.0:305.0] Now I'll start with the twist. Just to take up expectations in general.
[305.0:315.0] Optimization problems are solvable. This is a quote from the book of Uranistra.
[315.0:331.0] It's an exaggerated statement to make a point in the standpoint. Even if you have those form solutions in the end, the compute these solutions by computer, which we have to use finite precision or approximations.
[331.0:341.0] So numerically curious to use any true. And oftentimes, you just have to be content with approximately optimal solutions.
[341.0:347.0] And let's just let's state what those are.
[347.0:359.0] So we would call a solution epsilon optimal in objective value. So here it's important that I'm emphasizing that it's optimal in objective value.
[359.0:365.0] If the optimum is within an epsilon of the minimum.
[365.0:373.0] Remember, by definition, S star is the minimum possible.
[373.0:383.0] And any other solution must be about it. And we think about finding the solutions that get within this epsilon's track.
[383.0:395.0] So if you think about it visually, I'm not sure if this is visible. So let's say this is our S star.
[395.0:402.0] What we're doing is we're looking at comes back over.
[402.0:418.0] Next one. All right. And we're thinking about solutions that come within it. So in this case, it will be these sets.
[418.0:434.0] I'm not sure if this is visible. This is like an eye test for the audience. I think I'm pretty sure this also an eye test for the zoom attendees because it's like a little video unless you switch. It seems blind and my video.
[434.0:444.0] All right. Now, we would say that a solution is epsilon optimal in sequence.
[444.0:453.0] If for some norm, you get within an epsilon of the optimal solution. So in this case, let's assume that the optimal solution is unique.
[453.0:465.0] Now, in this particular picture, this is the optimal solution that obtained or obtained is this optimal value.
[465.0:477.0] So what this means is that you get basically within epsilon of this one.
[477.0:484.0] So this would be this particular set. And as you can see from the figure, right?
[484.0:499.0] This epsilon meaning epsilon optimal in sequence is considered a bit stronger than epsilon optimal in objective value.
[499.0:502.0] All right. Let's continue.
[502.0:520.0] So let's talk about a basic strategy that is that's the thing that makes money nowadays. You get a solution. You then refine this solution based on some information from the optimization problem.
[520.0:527.0] And then we try to make things better by refining it.
[527.0:542.0] Okay. So here's the basic template for what I just said. Let's start with a parameter initial parameter set, right? X0, which is in the domain of the function you're optimizing.
[542.0:548.0] And what we would like to do is basically generate a sequence of points X1, X0 dot dot dot.
[548.0:565.0] So that at every set, we somehow improve on your objective. I mean, it's not what all the standards do, but here's a stylized template that kind of makes sense because you would like to minimize an objective.
[565.0:571.0] And hence it makes sense to improve on your objective as you take steps.
[571.0:587.0] Now, in a very basic fashion, what we can do is write this particular sequence in the following way. Right. So let's say you have an XK at iteration count K.
[587.0:594.0] What you're doing is you refining it with maybe you go in some direction to this website.
[594.0:604.0] And you obtain your next iterate. And ideally, the objective value for this iterate is a little bit better than the previous one.
[604.0:608.0] Does that make sense?
[608.0:619.0] Okay. So then the name of the game is that how do we figure out this descent direction, right? That will lead to descend in the objective value.
[619.0:627.0] And how do you figure out the set for this descent direction?
[627.0:632.0] Now, there are some remarks that are do here. Right.
[632.0:638.0] The way you in general get this descent direction is by looking at the objective function.
[638.0:647.0] And the objective function will give you some information, such as its value, which you're particularly interested in.
[647.0:653.0] It's gradient, it's Haitian, and even more. Right.
[653.0:663.0] Now, depending on the information you get, you would call these algorithms maybe first order methods, second order methods, and whatever. Right.
[663.0:669.0] What that means is that if you get a, let's say you think about the Taylor series extension of the objective value.
[669.0:678.0] The first Taylor series extension is just the function stuff that is called a zero order term.
[678.0:684.0] You then get, so what I mean by the first limit of confused the audience by this.
[684.0:686.0] So think about it as follows.
[686.0:710.0] It's a simple Taylor series extension.
[710.0:719.0] So the first, what I mean here is that if you do the Taylor series extension for a particular perturbation.
[719.0:727.0] If you just get this very first term, meaning you evaluate the objective, that is a zero order method.
[727.0:735.0] If you get the gradient that is called the first order method, which means the set first order approximation, which is picked up the perturbation.
[735.0:741.0] Or the second order term, that would be a second order methods.
[741.0:752.0] And these values, we call them Oracle information because it's this, there's some Oracle that gives you this information.
[752.0:762.0] Good. So the choice of the Oracle meaning whether or not we choose to observe just the value or the gradient or the Haitian.
[762.0:771.0] Will determine our convergence rate overall efficiency and overall complexity of course.
[771.0:783.0] Good. Now let's think about what conditions we want in terms of the say, the direction that we go towards.
[783.0:792.0] So here is again the same expression. We would like to just move with the direction with the step size.
[792.0:796.0] So here is the Taylor series extension of x k plus 1 around x k.
[796.0:804.0] And as you will see that the first order term just takes the inner product between the gradient and this direction p.
[804.0:808.0] And there will be a second order term.
[808.0:820.0] Now if alpha is small enough, meaning if you just divide this by alpha and you let alpha go to 0, this term will disappear because this has alpha squared.
[820.0:830.0] You divide it by alpha. You have alpha left. If you let alpha go to 0, this term will disappear but this term will not.
[830.0:833.0] You can see the mouse pointer.
[833.0:843.0] Perfect. Good. So that's what I mean by an alpha is small enough. This term will dominate that term.
[843.0:857.0] So if you want this objective value in the next iteration to be smaller than the current objective value, what you need is this particular inner product to be negative.
[857.0:869.0] So that you have whatever value you have, you subtract remember step size can only be positive or non-negative in this particular case.
[869.0:875.0] So let's say it's positive. This needs to be negative.
[875.0:881.0] Does that make sense?
[881.0:901.0] Now we can actually do something even further than this. So if you think about it, this particular inner product is basically the norm of the gradient times the norm of the direction times the end limit.
[901.0:916.0] So if you want to have the maximum decrease, what do we need to do? We need to pick the angle so that the direction we do is negative like completely 180 degree.
[916.0:924.0] It's the opposite. The polar opposite of where the gradient is pointing.
[924.0:934.0] In this particular case, that would be the negative gradient. If you want the largest negative value.
[934.0:943.0] Does that make sense? In terms of the direction. Is this clear?
[943.0:964.0] But remember, in general, you can pick other directions because the locally best direction is not necessarily globally best direction because if you look at this particular looks for it, if you could pick a direction that just points directly to the optimum, which is here,
[964.0:973.0] then it is very best. So what we normally say is that you pick a descent direction that is in the element of the descent cone.
[973.0:982.0] So in this particular case, all you need to do is look at the tangent hyperplane and anything within this particular cone is okay.
[982.0:993.0] You decrease the objective as a direction. But if you want to have the maximum decrease locally, you want to pick the negative gradient direction.
[993.0:1003.0] So these arrows are possible directions. So let me repeat the question.
[1003.0:1011.0] What are these blue arrows? These blue arrows are directions that pk can be.
[1011.0:1029.0] Because if you want to just satisfy the rule that f of xk plus 1 is less than f of xk, you can pick any of these directions. I apologize for this technical difficulties. Hopefully I will figure this thing out.
[1029.0:1044.0] So you can pick this direction. You will have a decrease. You will pick that direction. You can have a decrease with the proper steps.
[1044.0:1051.0] So remember, here is a picture, which I think is intuitive, but let me tell you what this picture is.
[1051.0:1058.0] These are the level steps of the function. As you go down, you obtain the optimal objective.
[1058.0:1068.0] If you think about it, if you start from this particular point x0, the really best direction to go is the one that points to the optimal objective.
[1068.0:1077.0] The optimal, the optimal of the problem.
[1077.0:1088.0] If you are to pick the right step size, you can just get there one step. Fantastic. But we don't have this information.
[1088.0:1100.0] Let's say locally only obtain the gradient. What you can do is go towards the negative of this gradient with some step size.
[1100.0:1112.0] Let's say we pick this size so that we come here. And we can repeat this procedure until we reach x0.
[1112.0:1122.0] Does that make sense? It is exactly what I've been trying to describe, but the picture.
[1122.0:1133.0] Here is the deal. What we are trying to do in the end, do not forget about the statistical learning theory context here.
[1133.0:1140.0] Remember, let's say in the parametric setting, we have a true unknown parameter you would like to learn.
[1140.0:1148.0] We set up an estimator using maximum likelihood. We can set up the estimator using an M estimator.
[1148.0:1155.0] Now, this estimator's performance, its distance to the parameter depends on the data size typically.
[1155.0:1166.0] For the maximum likelihood estimator, we saw that this system goes down with p divided by n, where p is the ambient dimension of the parameter.
[1166.0:1180.0] And if you take more data, you can make this error go down to arbitrarily small. I'm just reminding you guys.
[1180.0:1197.0] x star, minus x2, where this was the maximum likelihood estimator was order p divided by n.
[1197.0:1209.0] We can make that small. But remember, an estimator is not an algorithm. It's a criteria.
[1209.0:1216.0] So this assumes that you have the perfect solution of the estimator.
[1216.0:1228.0] So, pictorials begin. If this was our true parameter, this x star, its distance, is what is called as the statistical error.
[1228.0:1234.0] It's got nothing to do with computation. There's not an algorithm.
[1234.0:1249.0] So, this is literally a criteria that you optimize. But by definition, a solution must satisfy this, which we can make small by just collecting more data.
[1249.0:1257.0] So, we can make the distance small by collecting data.
[1257.0:1266.0] But in general, how do we get this? We get the estimator solution by running an algorithm. Like the one that I just showed, the gradient descent.
[1266.0:1277.0] We take, we look at the objective function for our estimator. You look at this gradient. We compute the gradient using its form, the data, and so on and so forth.
[1277.0:1293.0] And we have, let's say an estimate, an editoration k, then we improve it by another one and so on and so forth. So, let's say the practical performance at time t is our numerical approximation to the estimator.
[1293.0:1304.0] But in the end, what we really care is that our performance in estimating the true parameter and not the estimator.
[1304.0:1312.0] Remember, we set up the estimator for the goal of estimating an unknown true parameter.
[1312.0:1319.0] You're running an algorithm. But the hopes of getting that true parameter.
[1319.0:1327.0] I mean, it's nice to get the estimator, but if the estimator is wrong, why do we care?
[1327.0:1339.0] So this is what I would like to highlight here. There's a subtle point, but it's an important one. Because if you think about it, there's actually a very nice error, the composition that you can think of.
[1339.0:1354.0] The error of our numerical computation to the true parameter at time t can be decomposed into two terms where this algorithm is trying to obtain a solution to this estimator.
[1354.0:1361.0] And the estimator is trying to obtain a solution to this true parameter.
[1361.0:1367.0] So this error of x t to x star is our numerical error.
[1367.0:1374.0] Because we now have an objective function. We query this objective function to get gradients.
[1374.0:1379.0] And we can evaluate our numerical precision somehow.
[1379.0:1388.0] So if you think about it, this is a very nice point.
[1388.0:1397.0] In general, we can see that our optimization algorithm trying to get close to x star.
[1397.0:1404.0] And our x star is trying to get close to this true parameter.
[1404.0:1421.0] So this decomposition will give you a triangle inequality. An easier thing, you know, our error in getting the true parameter is the summation of this, which is our numerical error.
[1421.0:1433.0] And this, which is our statistical error. But in reality, our true error might be much smaller.
[1433.0:1440.0] And in fact, you will see that in the literature, for example, people like to stop their algorithms earlier.
[1440.0:1446.0] That's okay, because you can actually be closer to the true parameter even if you stop earlier.
[1446.0:1455.0] And you will see that people talk about finding low to medium accuracy solutions for manual statistical learning problems.
[1455.0:1463.0] That also makes sense. Why? Because if your estimator is not a precise estimator, why do you need to over optimize?
[1463.0:1476.0] Because if here you continue doing computation and get closer and closer to the true, the estimator value, you're in fact getting farther and farther away from the true parameter.
[1476.0:1479.0] It's important.
[1479.0:1489.0] Okay. Now let's try to continue with the challenges to each of the optimization algorithms a bit further.
[1489.0:1498.0] And then it's just like, I like this particular picture because the way you do optimization is literally like you're playing Warcraft or Starcraft.
[1498.0:1504.0] There's like a Fogor 4. You can only open certain areas as you move.
[1504.0:1509.0] So here's the deal. This is what it looks like to an optimization algorithm.
[1509.0:1514.0] You start at the initial point. There's an objective that you would like to optimize.
[1514.0:1526.0] And somebody tells you the main, maybe the objective value, maybe the gradients and so on and so forth, and you try to move in a way to improve.
[1526.0:1538.0] When you look at this particular picture, you're like, maybe I'm in fact at the optimum location because it looks very flat.
[1538.0:1546.0] This is what it looks like. Maybe in reality, you may be here at a stationary point that is not an optimum point.
[1546.0:1551.0] So here's the effect X star.
[1551.0:1556.0] There are many challenges. There could be discontinuities.
[1556.0:1564.0] You may be at an on the frasiable point. You may be at a local minima.
[1564.0:1569.0] So if you start here, how do we get there?
[1569.0:1571.0] It's difficult.
[1571.0:1577.0] Unless we have some assumptions about the objective formulations.
[1577.0:1589.0] If the objective function is adversarial, there's not much we can do.
[1589.0:1595.0] So let's refresh our memories about some basic concepts.
[1595.0:1600.0] So here's our setting. You would like to minimize the function.
[1600.0:1607.0] It's twice the frasiable. I want to activate an argument here.
[1607.0:1610.0] Can you take a note of this?
[1610.0:1615.0] We need an argument of the minimizer.
[1615.0:1619.0] Let's think about the gradient discuss method.
[1619.0:1625.0] What does this method do? You look at the gradient. You take a step size.
[1625.0:1630.0] Let's say we have a constant step.
[1630.0:1637.0] Now, give an objective value. You can have a first order stationary point.
[1637.0:1645.0] The definition of a first order stationary point is that it's gradient is equal to zero.
[1645.0:1650.0] If you think about the respect to the gradient method,
[1650.0:1657.0] if you start the gradient method at the first order stationary point,
[1657.0:1665.0] it will remain there. It will not move from there.
[1665.0:1668.0] Let's just see this.
[1668.0:1671.0] It's obvious, but here's a trivial statement.
[1671.0:1675.0] You take the gradient. You multiply it with minus alpha.
[1675.0:1681.0] You multiply it with minus alpha.
[1681.0:1687.0] You multiply it with minus alpha.
[1687.0:1692.0] It's obvious, but it's obvious.
[1692.0:1696.0] Let's see what happens.
[1696.0:1705.0] It could be just a flat location, a saddle point.
[1705.0:1709.0] If you were to start your gradient method here, it will stop there.
[1709.0:1716.0] Is this good?
[1716.0:1725.0] No response? Is this good?
[1725.0:1729.0] Nobody wants to speak.
[1729.0:1732.0] It is not good.
[1732.0:1739.0] Let's say a second order stationary point,
[1739.0:1743.0] or even a local minimum would be better.
[1743.0:1745.0] If you think about this,
[1745.0:1750.0] a second order stationary point is the place where the Hessian also curves up.
[1750.0:1755.0] It's positive definite. At least locally speaking,
[1755.0:1761.0] it is a point that is better than all the other points nearby.
[1761.0:1768.0] You can see this from the Taylor series extension.
[1768.0:1773.0] If this gradient is 0,
[1773.0:1778.0] you want the curvature to be positive.
[1778.0:1781.0] If you're at an optimum point,
[1781.0:1787.0] you can only increase your objective.
[1787.0:1789.0] Does that make sense?
[1789.0:1795.0] Remember the positive definite definition is that this
[1795.0:1801.0] for any u needs to be rated as 0.
[1801.0:1806.0] If you're at a second order stationary point,
[1806.0:1813.0] you can only increase your objective by moving your own.
[1813.0:1817.0] If the Hessian is negative,
[1817.0:1821.0] then you're at a local maximum.
[1821.0:1823.0] If the Hessian is equal to 0,
[1823.0:1825.0] you can be at a saddle point.
[1825.0:1829.0] You can be at a local minimum or a local maximum.
[1829.0:1834.0] That requires this for the investigation.
[1834.0:1837.0] Let's see the difficulty of the local minimum.
[1837.0:1840.0] Here is an objective.
[1840.0:1845.0] Here is the gradient.
[1845.0:1848.0] Let's start at x0 is equal to 0.
[1848.0:1852.0] Let's pick a test size and you optimize.
[1852.0:1855.0] If you optimize, you will converge with a local minimum.
[1855.0:1858.0] Not the global minimum.
[1858.0:1860.0] If you start from this location,
[1860.0:1864.0] then you will go to the global minimum.
[1864.0:1868.0] It's a pure luck that you start from one location
[1868.0:1871.0] with initializations.
[1871.0:1875.0] You go to a local.
[1875.0:1878.0] Just to formalize this concept,
[1878.0:1884.0] what's the local minimum?
[1884.0:1887.0] Local minimum is the point
[1887.0:1890.0] where locally speaking,
[1890.0:1894.0] for all x within some x star,
[1894.0:1897.0] x star is just truly the minimum.
[1897.0:1902.0] x star is the bus in that neighborhood.
[1902.0:1905.0] It's the minimum.
[1905.0:1908.0] If you remember,
[1908.0:1912.0] in the last presentation,
[1912.0:1915.0] I talked about this convexity.
[1915.0:1918.0] I gave you four different definitions.
[1918.0:1921.0] Five different definitions of it.
[1921.0:1924.0] The funny thing about convexity is that
[1924.0:1928.0] if you have a convexity and you have a local minimum,
[1928.0:1932.0] that is indeed the global minimum.
[1932.0:1936.0] If you are optimizing a convexity,
[1936.0:1939.0] and if you were to find a local minimum,
[1939.0:1942.0] you have the global.
[1942.0:1945.0] It's very important.
[1945.0:1948.0] Comment or question.
[1948.0:1951.0] Mix.
[1951.0:1962.0] We will talk about this in the next slides.
[1962.0:1964.0] Thanks for the comment.
[1964.0:1970.0] Let's focus on the point at state here.
[1970.0:1973.0] Here's the point.
[1973.0:1975.0] You have a convexity function.
[1975.0:1978.0] The next star is the local minimum,
[1978.0:1981.0] but not global.
[1981.0:1983.0] Here, with one line,
[1983.0:1987.0] you can show that that leads to contradiction.
[1987.0:1992.0] If you remember the convexity,
[1992.0:1996.0] if you take some physical combinations,
[1996.0:2007.0] the function needs to be below the line.
[2007.0:2010.0] This is true for convex functions.
[2010.0:2013.0] Here,
[2013.0:2016.0] I will say x1,
[2016.0:2018.0] x2,
[2018.0:2022.0] x1, x2,
[2022.0:2024.0] x1, x2.
[2024.0:2031.0] It needs to be less than or equal to alpha times alpha times x1,
[2031.0:2035.0] minus alpha times alpha times x2.
[2035.0:2041.0] This definition of convexity.
[2041.0:2045.0] Let's pick any x.
[2045.0:2048.0] If x star is a local minimum,
[2048.0:2050.0] there exists a global minimum,
[2050.0:2053.0] and let's take x is the global minimum.
[2053.0:2056.0] Let's take this up here.
[2056.0:2065.0] I just write down the definition of convexity.
[2065.0:2067.0] Now, I'll prove this.
[2067.0:2070.0] This is obviously f of x star,
[2070.0:2073.0] because f of x is less than or equal to f of x star.
[2073.0:2076.0] This is a contradiction.
[2076.0:2083.0] By taking a very small alpha locally moving x star,
[2083.0:2086.0] I can make a smaller objective,
[2086.0:2089.0] which contradicts itself,
[2089.0:2095.0] that x star is a local optimum.
[2095.0:2097.0] In particular,
[2097.0:2099.0] for any convex,
[2099.0:2101.0] the function, any stationary point,
[2101.0:2104.0] is a global minimum.
[2104.0:2108.0] The effect of the step size,
[2108.0:2111.0] let's think about that.
[2111.0:2117.0] Here,
[2117.0:2119.0] here's an objective,
[2119.0:2120.0] this is convex,
[2120.0:2122.0] quadratic,
[2122.0:2123.0] gradient,
[2123.0:2125.0] if you pick a step size that is small,
[2125.0:2127.0] you will see that the sequence
[2127.0:2131.0] will converge to the optimum very slowly.
[2131.0:2133.0] On the other hand,
[2133.0:2136.0] the value of the step size is very large.
[2136.0:2141.0] You will start going away from the objective.
[2141.0:2143.0] With regards to the previous comment
[2143.0:2146.0] about the w function and picking a largest step size,
[2146.0:2148.0] yes, if you start from 0
[2148.0:2151.0] and maybe pick a bit larger step size,
[2151.0:2153.0] then you may go to the global,
[2153.0:2155.0] but if you start from 1
[2155.0:2157.0] and you were going to go to the global
[2157.0:2159.0] and pick a large step size,
[2159.0:2161.0] you may go to the local,
[2161.0:2165.0] the global,
[2165.0:2168.0] which is very large.
[2168.0:2173.0] Anything can happen.
[2173.0:2175.0] Here's another challenge,
[2175.0:2177.0] just continued these.
[2177.0:2178.0] Often times,
[2178.0:2181.0] we try to minimize things
[2181.0:2184.0] with respect to some constraints.
[2184.0:2186.0] Maybe some safety constraints
[2186.0:2189.0] that we cannot violate.
[2189.0:2196.52] parameters. If you're trying to optimize some parameters of a vehicle turning at the curve,
[2197.24:2202.92] you don't want to make your speed higher than the speed that is safe. You want to put a
[2202.92:2210.12] heart constraint there and you want to optimize under the heart constraint. Okay. So if you think
[2210.12:2218.36] about it, so let's say this is our optimization domain, right, the shaded area. So this is not
[2218.36:2227.2400000000002] allowed, right? So the optimum here is not this location. The optimum here is this location.
[2229.32:2236.04] All right. So for here here, our x star is here. The gradient is pointing out this direction.
[2236.04:2240.44] The negative gradient points out this direction. We cannot even move.
[2240.44:2251.7200000000003] All right. It's not allowed. All right. Instead of, so here's another question. Instead of
[2251.7200000000003:2257.48] picking learning rates, would having a momentum weight, previous gradient help more to jump over
[2257.48:2264.12] this force, separating local and global. This is like a vacuum mold. It is interesting to
[2264.12:2270.2] imagine momentum will help. But if you fix this strategy, I will give you a function where you
[2270.2:2281.3199999999997] will just not get the globe. All right. For the time being, let's focus on the algorithmic aspects.
[2281.3199999999997:2286.92] I think maybe we can talk about some of these myths in my opinion. Let me talk about deep learning.
[2286.92:2295.32] All right. People like to justify the performance of an algorithm by saying things like,
[2295.32:2299.64] oh, yeah, the momentum helped and so on and so forth. But in reality, we have no idea.
[2300.6800000000003:2304.52] All right. I'm sorry, this is not an answer. It's more like a criticism of the question.
[2305.16:2313.64] But for the time being, let's hold on to jumping conclusions. All right. Whether or not momentum
[2313.64:2319.7999999999997] will help or picking a step size will help. Let's focus on the convergence characterizations.
[2320.7599999999998:2329.0] And then see what we can say afterwards. Okay. All right. Non-sum of functions.
[2331.4:2335.0] So when you have a non-sum of function, the gradient is not uniquely defined.
[2335.7999999999997:2342.6] All right. This is an issue. And then you also have non-com mixed functions,
[2342.6:2347.64] even getting like properly defining the subgradient is also a difficult one. Because if you recall
[2347.64:2352.7599999999998] from the recitation, it defined the, for the convex case, it was easy to define. All right.
[2353.72:2360.7599999999998] We would look at only this first order term. And we would say that any direction that satisfies this
[2360.76:2373.4] inequality would be a subgradient. But if you have a non-com mixed function, right. So let's say this
[2373.4:2383.0] w here, locally speaking, this is the subgradient, but it violates the function. It crosses the function.
[2384.1200000000003:2389.1600000000003] So you need to locally define subgradients and so forth. In general, you have a king,
[2389.16:2395.48] any hyperplane under that king, you will give your subgradient. All right.
[2399.64:2406.2799999999997] So in this particular case, what you can do is define what is called as the subgradient
[2406.28:2418.52] methods. Okay. Meaning you have a current point. You obtain a subgradient from the sub differential,
[2418.52:2425.8] right. The set of points that satisfy this is called the sub differential. You pick an element
[2425.8:2431.48] from that sub differential. That's our subgradient. An element from the set of sub differential.
[2431.48:2443.0] All right. In this case, we can argue maybe that somehow with the set by a procedure,
[2443.0:2447.64] with proper set by procedure, X cable, conversion, the station point. Okay.
[2450.36:2454.92] You know, in this particular case, here's an example. Here's the absolute value.
[2454.92:2463.48] All right. The subgradient of the absolute value function, when X is equal to zero,
[2464.28:2469.48] it's the sign of the gradients. So in general, it's the sign of the gradient,
[2469.48:2475.8] but it can be anything between minus one and one. All right. So if you think about it,
[2477.32:2483.08] if somebody is giving you the subgradients, you're not picking them. Somebody is giving them to you.
[2483.08:2491.24] Okay. Suppose you start the optimization. So here's a function that has a, let's say,
[2491.24:2500.2799999999997] it's like a destructured function, okay. For welcome. V for welcome. All right. So let's say you're at
[2500.2799999999997:2509.08] the optimum location. You're at the optimum. You're good. But somebody else is giving you subgradients,
[2509.08:2515.0] and they can pick subgradients in these directions. Remember, in the V, you can pick
[2517.88:2525.48] any subgradients, right? So I can give you non-zero subgradient. I can give you zero subgradient too.
[2527.64:2534.36] But I'm not a nice person. I give you non-zero subgradient. And you will take this,
[2534.36:2542.92] you will put a test size, and then you will move out of the optimum. You were at the optimum,
[2542.92:2552.36] you moved out of the optimum. It's something clearly you don't want. All right. So you will see
[2552.36:2556.04] for non-zero problems where you obtain solutions with the subgradient descent,
[2556.04:2562.44] the test size often needs to decrease. And there are some questions here. Is the subgradient
[2562.44:2569.2400000000002] an element of sub differential? This is correct. Sub differential is the set of points that satisfy
[2569.96:2575.32] that inequality. When you pick an element from that set, you would call it a subgradient,
[2575.32:2581.8] that is correct. Could you please help me better understand the notion of subgradient? I don't
[2581.8:2588.52] think I got it yet. Thank you. So first, did you, did you see recitation too? Because we
[2588.52:2596.2] discussed subgradients there in great detail for complex functions. All right. I'll just tell one more
[2596.2:2604.52] time. And then actually maybe we take this as a break now. I'll just quit the talk. So we take
[2604.52:2616.6] a break because it's 11 o'clock now. All right. I'll tell you maybe here. Maybe I talk about
[2616.6:2620.2799999999997] the set to beginning off the class so that everybody listens. All right. Let's take a 15 minute break.
[2620.28:2645.5600000000004] We started 11.15.
